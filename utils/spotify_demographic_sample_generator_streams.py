import argparse
import pandas as pd
import numpy as np
import os

# Set up logging
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Added format
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.info("Starting demographic generation process using synthetic interaction data...")

# --- Configuration ---
USER_HISTORY_FILE = 'data/processed/synthetic_user_history.csv'
MUSIC_INFO_FILE = 'data/processed/spotify_tracks_with_ids.csv'
OUTPUT_COMPLETE_DATASET = 'data/processed/spotify_complete_dataset.csv'
OUTPUT_USER_DEMOGRAPHICS = 'data/processed/spotify_user_demographics.csv'
# ---------------------

def parse_args():
    parser = argparse.ArgumentParser(description="Generate user demographics from synthetic interactions.")
    parser.add_argument("--user-history", default=USER_HISTORY_FILE)
    parser.add_argument("--music-info", default=MUSIC_INFO_FILE)
    parser.add_argument("--output-complete", default=OUTPUT_COMPLETE_DATASET)
    parser.add_argument("--output-demo", default=OUTPUT_USER_DEMOGRAPHICS)
    parser.add_argument("--seed", type=int, help="Random seed for reproducibility")
    return parser.parse_args()

# Load the datasets generated by generate_synthetic_interactions.py
args = parse_args()
if args.seed is not None:
    np.random.seed(args.seed)
USER_HISTORY_FILE = args.user_history
MUSIC_INFO_FILE = args.music_info
OUTPUT_COMPLETE_DATASET = args.output_complete
OUTPUT_USER_DEMOGRAPHICS = args.output_demo

try:
    users_history = pd.read_csv(USER_HISTORY_FILE)
    music_info = pd.read_csv(MUSIC_INFO_FILE)
except FileNotFoundError as e:
    logger.error(f"Error loading input files: {e}. Please run generate_synthetic_interactions.py first.")
    exit() # Exit if files are missing
except Exception as e:
    logger.error(f"Error reading input CSV files: {e}")
    exit()

logger.info("User history shape: %s", users_history.shape)
logger.info("User history columns: %s", users_history.columns.tolist()) # <-- Added log
logger.info("Music info shape: %s", music_info.shape)
logger.info("Music info columns: %s", music_info.columns.tolist()) # <-- Added log

# --- Removed initial track filtering and user sampling ---
# This is now handled by the synthetic data generation script.
# We assume the loaded data is already the desired sample/set.

# Check for missing values in the music info dataset
missing_values = music_info.isnull().sum()
logger.info("Missing values in music info dataset:\n%s", missing_values[missing_values > 0])
# Handle potential missing tags if necessary before genre extraction
music_info['tags'].fillna('unknown', inplace=True) # Fill NaN tags

# Extract genre information - create a simplified genre map
def extract_main_genre(tag_string):
    if pd.isna(tag_string):
        return "unknown"

    tags = str(tag_string).lower().split(",")
    # Define genre hierarchy with primary genres
    genre_map = {
        "rock": ["rock", "metal", "punk", "alternative", "grunge", "hard rock", "classic rock"],
        "electronic": ["electronic", "edm", "house", "techno", "dubstep", "trance"],
        "hip_hop": ["hip-hop", "rap", "trap", "drill"],
        "pop": ["pop", "dance pop", "k-pop", "electropop", "synth pop"],
        "classical": ["classical", "baroque", "orchestra", "piano"],
        "jazz": ["jazz", "bebop", "fusion", "blues", "funk"],
        "folk": ["folk", "acoustic", "singer-songwriter", "bluegrass", "americana"],
        "latin": ["latin", "reggae", "salsa", "bachata", "afrobeat"],
        "rb_soul": ["r&b", "soul", "motown"],
        "country": ["country", "bluegrass", "americana"],
        "religious": ["christian", "gospel", "worship", "spiritual"],
    }

    # Check for matches
    for main_genre, sub_genres in genre_map.items():
        if any(sub in tag for sub in sub_genres for tag in tags):
            return main_genre

    return "other"

# Add main genre to music info
logger.info("Extracting main genres...")
# Ensure 'tags' column is string type before applying split
music_info["main_genre"] = music_info["tags"].astype(str).apply(extract_main_genre)

# Merge user history with full music info to get all required columns
logger.info("Merging user history with music info...")
# Ensure track_id types match before merging
users_history['track_id'] = users_history['track_id'].astype(music_info['track_id'].dtype)

# Rename columns before merge to avoid conflicts
users_history = users_history.rename(columns={'playcount': 'playcount'})
music_info = music_info.rename(columns={'playcount': 'track_popularity'})

full_user_tracks = users_history.merge(
    music_info,
    on="track_id",
    how="inner"
)

final_dataset = full_user_tracks  # Alias for compatibility with downstream quality checks

logger.info("Columns after merge (full_user_tracks): %s", full_user_tracks.columns.tolist()) # <-- Added log

# Check if merge resulted in empty dataframe
if full_user_tracks.empty:
    logger.error("Merge between user history and music info resulted in an empty DataFrame. Check track_ids.")
    exit()

# Create user preference profile based on genres and all audio features
logger.info("Building user preferences...")

# List of all audio features to include (ensure these columns exist in music_info)
audio_features = [
    "danceability", "energy", "key", "loudness", "mode",
    "speechiness", "acousticness", "instrumentalness",
    "liveness", "valence", "tempo", "time_signature"
]

# Verify which audio features are actually in the dataset
available_features = [feat for feat in audio_features if feat in music_info.columns]
if len(available_features) < len(audio_features):
    missing_features = set(audio_features) - set(available_features)
    logger.warning(f"Missing audio features in dataset: {missing_features}")
    logger.info(f"Using available audio features: {available_features}")

# Get genre preferences for each user (with simplified processing)
# Check if 'playcount' exists before grouping
if 'playcount' not in full_user_tracks.columns:
    logger.error(f"Critical Error: 'playcount' column is missing from full_user_tracks DataFrame after merge.")
    logger.error(f"Available columns: {full_user_tracks.columns.tolist()}")
    exit()

user_genre_counts = full_user_tracks.groupby(["user_id", "main_genre"])["playcount"].sum()
# Handle potential multi-index issues if a user has no history after merge
if user_genre_counts.empty:
    logger.warning("No genre counts generated. Check the merged data.")
    # Create an empty DataFrame with expected structure if needed later
    user_genre_prefs = pd.DataFrame(columns=['user_id', 'top_genre']).set_index('user_id')
else:
    user_genre_prefs = user_genre_counts.unstack(fill_value=0)
    # Normalize to get percentages
    row_sums = user_genre_prefs.sum(axis=1)
    # Avoid division by zero if a user has zero total playcount (shouldn't happen with synthetic data)
    row_sums[row_sums == 0] = 1
    user_genre_prefs = user_genre_prefs.div(row_sums, axis=0) * 100
    user_genre_prefs.fillna(0, inplace=True)
    # Add top genre column
    user_genre_prefs["top_genre"] = user_genre_prefs.idxmax(axis=1)
    user_genre_prefs["top_genre"] = user_genre_prefs["top_genre"].fillna("unknown")


# Get average audio features for each user (optimized implementation)
user_audio_features = pd.DataFrame(index=user_genre_prefs.index) # Use index from genre prefs
user_total_playcounts = full_user_tracks.groupby("user_id")["playcount"].sum()
user_audio_features["total_playcount"] = user_total_playcounts

# Calculate weighted averages for audio features
for feature in available_features:
    if feature in full_user_tracks.columns and not full_user_tracks[feature].isnull().all(): # Check if feature exists and has non-NA values
        # Ensure feature is numeric before calculation
        full_user_tracks[feature] = pd.to_numeric(full_user_tracks[feature], errors='coerce')
        # Handle potential NaNs introduced by coercion or already present
        full_user_tracks_feature_no_na = full_user_tracks.dropna(subset=[feature])

        if not full_user_tracks_feature_no_na.empty:
             # Calculate weighted sum for the feature
             weighted_feature_sum = (full_user_tracks_feature_no_na["playcount"] * full_user_tracks_feature_no_na[feature])\
                                     .groupby(full_user_tracks_feature_no_na["user_id"]).sum()

             # Get the corresponding total playcount for users who have non-NA values for this feature
             relevant_playcounts = user_total_playcounts.loc[weighted_feature_sum.index]
             relevant_playcounts[relevant_playcounts == 0] = 1 # Avoid division by zero

             # Calculate weighted average
             user_audio_features[f"avg_{feature}"] = weighted_feature_sum / relevant_playcounts
        else:
             logger.warning(f"Feature '{feature}' has all NaN values after coercion. Skipping average calculation.")
             user_audio_features[f"avg_{feature}"] = np.nan # Assign NaN if feature is all NaN

# Fill any remaining NaNs in avg features (e.g., for users with no valid feature data) with 0 or mean? Using 0 for now.
user_audio_features.fillna(0, inplace=True)

# Reset index and merge user features
user_genre_prefs = user_genre_prefs.reset_index()
user_audio_features = user_audio_features.reset_index()

# Check if user_ids are compatible for merge
if 'user_id' not in user_genre_prefs.columns or 'user_id' not in user_audio_features.columns:
     logger.error("Missing 'user_id' column in intermediate dataframes before merging features.")
     exit()

user_features = user_genre_prefs.merge(user_audio_features, on="user_id", how="left") # Use left merge to keep all users from genre prefs
# Fill NaNs that might result from the merge (if a user had genres but somehow no audio features calculated)
user_features.fillna({"total_playcount": 0, "top_genre": "unknown"}, inplace=True)
for feature in available_features:
     user_features.fillna({f"avg_{feature}": 0}, inplace=True)


# Define simplified demographic correlations
genre_age_map = {
    "rock": [0.15, 0.30, 0.25, 0.15, 0.10, 0.05],  # More middle-aged
    "electronic": [0.40, 0.35, 0.15, 0.05, 0.03, 0.02],  # Young adult
    "hip_hop": [0.45, 0.30, 0.15, 0.05, 0.03, 0.02],  # Youngest demographic
    "pop": [0.35, 0.30, 0.15, 0.10, 0.05, 0.05],  # Younger skew
    "classical": [0.10, 0.15, 0.15, 0.20, 0.20, 0.20],  # Oldest demographic
    "jazz": [0.05, 0.15, 0.20, 0.25, 0.20, 0.15],  # Older demographic
    "folk": [0.15, 0.20, 0.20, 0.20, 0.15, 0.10],  # Balanced but older
    "latin": [0.30, 0.30, 0.20, 0.10, 0.05, 0.05],  # Younger adult
    "rb_soul": [0.20, 0.25, 0.25, 0.15, 0.10, 0.05],  # Mixed age groups
    "country": [0.15, 0.20, 0.25, 0.20, 0.15, 0.05],  # Middle-aged skew
    "religious": [0.15, 0.20, 0.20, 0.20, 0.15, 0.10],  # Balanced distribution
    "other": [0.30, 0.30, 0.15, 0.10, 0.10, 0.05],  # Generic distribution
    "unknown": [0.30, 0.30, 0.15, 0.10, 0.10, 0.05],  # Generic distribution
}

genre_gender_map = {
    "rock": {"Male": 0.60, "Female": 0.40},
    "electronic": {"Male": 0.60, "Female": 0.40},
    "hip_hop": {"Male": 0.55, "Female": 0.45},
    "pop": {"Male": 0.35, "Female": 0.65},
    "classical": {"Male": 0.48, "Female": 0.52},
    "jazz": {"Male": 0.55, "Female": 0.45},
    "folk": {"Male": 0.45, "Female": 0.55},
    "latin": {"Male": 0.40, "Female": 0.60},
    "rb_soul": {"Male": 0.45, "Female": 0.55},
    "country": {"Male": 0.48, "Female": 0.52},
    "religious": {"Male": 0.45, "Female": 0.55},
    "other": {"Male": 0.50, "Female": 0.50},
    "unknown": {"Male": 0.50, "Female": 0.50},
}

genre_region_map = {
    "rock": {
        "Europe": 0.35,
        "North America": 0.35,
        "Latin America": 0.15,
        "Rest of World": 0.15,
    },
    "electronic": {
        "Europe": 0.40,
        "North America": 0.25,
        "Latin America": 0.15,
        "Rest of World": 0.20,
    },
    "hip_hop": {
        "Europe": 0.20,
        "North America": 0.45,
        "Latin America": 0.20,
        "Rest of World": 0.15,
    },
    "pop": {
        "Europe": 0.30,
        "North America": 0.30,
        "Latin America": 0.20,
        "Rest of World": 0.20,
    },
    "classical": {
        "Europe": 0.40,
        "North America": 0.30,
        "Latin America": 0.10,
        "Rest of World": 0.20,
    },
    "jazz": {
        "Europe": 0.30,
        "North America": 0.40,
        "Latin America": 0.15,
        "Rest of World": 0.15,
    },
    "folk": {
        "Europe": 0.35,
        "North America": 0.40,
        "Latin America": 0.10,
        "Rest of World": 0.15,
    },
    "latin": {
        "Europe": 0.15,
        "North America": 0.20,
        "Latin America": 0.55,
        "Rest of World": 0.10,
    },
    "rb_soul": {
        "Europe": 0.20,
        "North America": 0.45,
        "Latin America": 0.20,
        "Rest of World": 0.15,
    },
    "country": {
        "Europe": 0.15,
        "North America": 0.60,
        "Latin America": 0.15,
        "Rest of World": 0.10,
    },
    "religious": {
        "Europe": 0.25,
        "North America": 0.35,
        "Latin America": 0.25,
        "Rest of World": 0.15,
    },
    "other": {
        "Europe": 0.27,
        "North America": 0.28,
        "Latin America": 0.22,
        "Rest of World": 0.23,
    },
    "unknown": {
        "Europe": 0.27,
        "North America": 0.28,
        "Latin America": 0.22,
        "Rest of World": 0.23,
    },
}

# Define age groups
age_groups = [
    (18, 24),  # 31.51%
    (25, 34),  # 31.41%
    (35, 44),  # ~15%
    (45, 54),  # ~10%
    (55, 64),  # ~8%
    (65, 100),  # ~4%
]

# Use audio features to refine segmentation with all available features
def adjust_demographics(row):
    top_genre = row["top_genre"]
    
    # Get base probabilities from genre
    age_probs = genre_age_map.get(top_genre, genre_age_map["other"]).copy()
    gender_probs = genre_gender_map.get(top_genre, genre_gender_map["other"]).copy()
    region_probs = genre_region_map.get(top_genre, genre_region_map["other"]).copy()

    # Adjust based on audio features if available
    if "avg_energy" in row and pd.notna(row["avg_energy"]) and row["avg_energy"] > 0.7:
         for i in range(2):  # Boost first two age groups
             age_probs[i] *= 1.2

    if "avg_valence" in row and pd.notna(row["avg_valence"]) and row["avg_valence"] > 0.6:
         gender_probs["Female"] *= 1.1

    if "avg_danceability" in row and pd.notna(row["avg_danceability"]) and row["avg_danceability"] > 0.7:
         region_probs["Latin America"] *= 1.2
         region_probs["Europe"] *= 1.1
        
    if "avg_acousticness" in row and pd.notna(row["avg_acousticness"]) and row["avg_acousticness"] > 0.7:
         for i in range(3, 6):  # Boost older age groups
             age_probs[i] *= 1.15
             
    if "avg_speechiness" in row and pd.notna(row["avg_speechiness"]) and row["avg_speechiness"] > 0.3:
         for i in range(2):  # Boost youngest age groups
             age_probs[i] *= 1.2
         region_probs["North America"] *= 1.1

    # Normalize all distributions
    age_probs = np.array(age_probs)/sum(age_probs)
    gender_probs = {k:v/sum(gender_probs.values()) for k,v in gender_probs.items()}
    region_probs = {k:v/sum(region_probs.values()) for k,v in region_probs.items()}

    # Sample
    age_group = age_groups[np.random.choice(len(age_groups), p=age_probs)]
    gender = np.random.choice(list(gender_probs.keys()), p=list(gender_probs.values()))
    region = np.random.choice(list(region_probs.keys()), p=list(region_probs.values()))
    country = np.random.choice(countries_by_region[region])
    return age_group, gender, region, country

# Create country lists by region
countries_by_region = {
    "Europe": ["UK", "Germany", "France", "Spain", "Italy", "Sweden", "Netherlands", "Poland"],
    "North America": ["USA", "Canada"],
    "Latin America": ["Brazil", "Mexico", "Argentina", "Colombia", "Chile"],
    "Rest of World": ["Japan", "Australia", "India", "South Korea", "South Africa", "UAE"],
}

# Generate demographics
logger.info("Generating demographics...")
demographics = []

# Check if user_features is empty before iterating
if user_features.empty:
    logger.warning("User features DataFrame is empty. No demographics will be generated.")
else:
    for _, row in user_features.iterrows():
        age_group, gender, region, country = adjust_demographics(row)
        demographics.append({
            'user_id': row['user_id'],
            'age_group': f"{age_group[0]}-{age_group[1]}",
            'gender': gender,
            'region': region,
            'country': country
        })

# Create final demographics dataframe
user_demographics = pd.DataFrame(demographics)

# Add numeric age column (midpoint of age range)
if not user_demographics.empty:
    # Extract min and max age from age_group and calculate mid-point
    user_demographics['age'] = user_demographics['age_group'].str.split('-').apply(
        lambda x: (int(x[0]) + int(x[1])) / 2 if isinstance(x, list) and len(x) == 2 else np.nan)
    
    # Merge with user_features to get the 'top_genre' column
    user_demographics = user_demographics.merge(
        user_features[['user_id', 'top_genre']], 
        on='user_id', 
        how='left'
    )

# Ensure output directories exist
os.makedirs(os.path.dirname(OUTPUT_USER_DEMOGRAPHICS), exist_ok=True)

# Save the user demographics only once
logger.info(f"Saving demographic file to {OUTPUT_USER_DEMOGRAPHICS}...")
if not user_demographics.empty:
    user_demographics.to_csv(OUTPUT_USER_DEMOGRAPHICS, index=False)
else:
    logger.warning("User demographics DataFrame is empty. Skipping save.")


# Display summary statistics (only if data exists)
if not user_demographics.empty:
    logger.info("\nDemographic Distribution Overview:")
    logger.info(f"Age: Mean={user_demographics['age'].mean():.1f}, Median={user_demographics['age'].median()}")

    logger.info("\nAge Groups:")
    age_groups_count = user_demographics['age'].value_counts(bins=[18, 24, 34, 44, 54, 64, 100]).sort_index()
    logger.info(f"{age_groups_count}")

    logger.info("\nGender Distribution:")
    gender_dist = user_demographics["gender"].value_counts(normalize=True) * 100
    logger.info(f"{gender_dist}")

    logger.info("\nRegion Distribution:")
    region_dist = user_demographics["region"].value_counts(normalize=True) * 100
    logger.info(f"{region_dist}")

    logger.info("\nTop Genres:")
    top_genres = user_demographics["top_genre"].value_counts().head(10)
    logger.info(f"{top_genres}")

# Display year distribution of songs used (from the loaded music_info)
logger.info("\nYear Distribution of Songs Used:")
year_desc = music_info["year"].describe()
logger.info(f"{year_desc}")

# Display audio features summary (only if data exists)
if not user_demographics.empty:
    logger.info("\nAudio Features Summary (User Averages):")
    for feature in available_features:
        if f"avg_{feature}" in user_demographics.columns:
            logger.info(f"{feature}: Mean={user_demographics[f'avg_{feature}'].mean():.4f}")

# Create and save the complete dataset by joining full_user_tracks with demographics
if not user_demographics.empty and not full_user_tracks.empty:
    logger.info("\nCreating complete dataset with user demographics...")
    final_complete_data = full_user_tracks.merge(
        user_demographics,
        on="user_id",
        how="left"
    )
    
    logger.info(f"Complete dataset shape: {final_complete_data.shape}")
    logger.info(f"Saving complete dataset to {OUTPUT_COMPLETE_DATASET}...")
    final_complete_data.to_csv(OUTPUT_COMPLETE_DATASET, index=False)
else:
    logger.warning("Cannot create complete dataset due to missing data.")

logger.info("\nProcess complete!")

# Calculate additional quality metrics for final dataset (only if data exists)
if not final_dataset.empty:
    logger.info("\nPerforming quality checks on final dataset...")

    # Check for missing values in the final dataset
    missing_values_final = final_dataset.isnull().sum()
    logger.info(f"Missing values in final dataset:\n{missing_values_final[missing_values_final > 0]}")

    # Check for numerical outliers in key columns
    for feature in available_features:
        if feature in final_dataset.columns:
            q1 = final_dataset[feature].quantile(0.25)
            q3 = final_dataset[feature].quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            outliers = final_dataset[(final_dataset[feature] < lower_bound) | (final_dataset[feature] > upper_bound)]
            if len(outliers) > 0:
                outlier_pct = (len(outliers) / len(final_dataset)) * 100
                logger.info(f"Outliers in {feature}: {len(outliers)} records ({outlier_pct:.2f}%)")

    # Check demographic distribution matches our expected probabilities (only if data exists)
    if not user_demographics.empty:
        logger.info("\nVerifying demographic distributions match expected probabilities...")        # Age distribution check
        age_group_data = []
        for i, (min_age, max_age) in enumerate(age_groups):
            group_name = f"{min_age}-{max_age}"
            # Match the string format in age_group column instead of using numeric comparison
            age_group_match = f"{min_age}-{max_age}"
            actual_count = len(user_demographics[user_demographics['age_group'] == age_group_match])
            actual_pct = (actual_count / len(user_demographics)) * 100
            
            # Calculate expected percentages (average across all genres)
            expected_pcts = [genre_age_map[genre][i] for genre in genre_age_map.keys()]
            expected_pct = np.mean(expected_pcts) * 100
            
            age_group_data.append({
                'group': group_name,
                'actual_pct': actual_pct,
                'expected_pct': expected_pct,
                'difference': actual_pct - expected_pct
            })

        age_distribution_check = pd.DataFrame(age_group_data)
        logger.info(f"Age distribution check:\n{age_distribution_check}")

        # Gender distribution check
        gender_actual = user_demographics['gender'].value_counts(normalize=True) * 100
        gender_expected = {gender: np.mean([probs[gender] for probs in genre_gender_map.values()]) * 100 
                          for gender in ['Male', 'Female']}
        logger.info(f"Gender distribution - Actual vs Expected:")
        for gender in gender_expected:
            logger.info(f"{gender}: Actual {gender_actual.get(gender, 0):.2f}% vs Expected {gender_expected[gender]:.2f}%")

    # Additional playcount analysis
    logger.info("\nPlaycount Statistics:")
    logger.info(f"Mean playcount per user-track: {final_dataset['playcount'].mean():.2f}")
    logger.info(f"Median playcount per user-track: {final_dataset['playcount'].median():.2f}")
    logger.info(f"Min playcount: {final_dataset['playcount'].min()}")
    logger.info(f"Max playcount: {final_dataset['playcount'].max()}")

    # Check for users with unusually high playcounts
    user_total_plays = final_dataset.groupby('user_id')['playcount'].sum()
    high_playcount_users = user_total_plays[user_total_plays > user_total_plays.quantile(0.95)]
    if len(high_playcount_users) > 0:
        logger.info(f"Users with unusually high playcounts (top 5%):")
        logger.info(f"{high_playcount_users.sort_values(ascending=False).head()}")

    logger.info("Quality checks complete!")
else:
    logger.warning("Final dataset is empty. Skipping quality checks.")
