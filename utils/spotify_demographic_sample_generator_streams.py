import numpy as np
import pandas as pd
import random
from collections import Counter
import logging
import os

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Added format
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.info("Starting demographic generation process using synthetic interaction data...")

# --- Configuration ---
USER_HISTORY_FILE = 'data/processed/synthetic_user_history.csv'
MUSIC_INFO_FILE = 'data/processed/spotify_tracks_with_ids.csv'
OUTPUT_COMPLETE_DATASET = 'reports_and_results/spotify_complete_dataset.csv'
OUTPUT_USER_DEMOGRAPHICS = 'reports_and_results/spotify_user_demographics.csv'
# ---------------------

# Load the datasets generated by generate_synthetic_interactions.py
try:
    users_history = pd.read_csv(USER_HISTORY_FILE)
    music_info = pd.read_csv(MUSIC_INFO_FILE)
except FileNotFoundError as e:
    logger.error(f"Error loading input files: {e}. Please run generate_synthetic_interactions.py first.")
    exit() # Exit if files are missing
except Exception as e:
    logger.error(f"Error reading input CSV files: {e}")
    exit()

logger.info("User history shape: %s", users_history.shape)
logger.info("User history columns: %s", users_history.columns.tolist()) # <-- Added log
logger.info("Music info shape: %s", music_info.shape)
logger.info("Music info columns: %s", music_info.columns.tolist()) # <-- Added log

# --- Removed initial track filtering and user sampling ---
# This is now handled by the synthetic data generation script.
# We assume the loaded data is already the desired sample/set.

# Check for missing values in the music info dataset
missing_values = music_info.isnull().sum()
logger.info("Missing values in music info dataset:\n%s", missing_values[missing_values > 0])
# Handle potential missing tags if necessary before genre extraction
music_info['tags'].fillna('unknown', inplace=True) # Fill NaN tags

# Extract genre information - create a simplified genre map
def extract_main_genre(tag_string):
    if pd.isna(tag_string):
        return "unknown"

    tags = str(tag_string).lower().split(",")

    # Define genre hierarchy with primary genres
    genre_map = {
        "rock": ["rock", "metal", "punk", "alternative", "grunge", "hard rock", "classic rock"],
        "electronic": ["electronic", "edm", "house", "techno", "dubstep", "trance"],
        "hip_hop": ["hip-hop", "rap", "trap", "drill"],
        "pop": ["pop", "dance pop", "k-pop", "electropop", "synth pop"],
        "classical": ["classical", "baroque", "orchestra", "piano"],
        "jazz": ["jazz", "bebop", "fusion", "blues", "funk"],
        "folk": ["folk", "acoustic", "singer-songwriter", "bluegrass", "americana"],
        "latin": ["latin", "reggae", "salsa", "bachata", "afrobeat"],
        "rb_soul": ["r&b", "soul", "motown"],
        "country": ["country", "bluegrass", "americana"],
        "religious": ["christian", "gospel", "worship", "spiritual"],
    }

    # Check for matches
    for main_genre, sub_genres in genre_map.items():
        if any(sub in " ".join(tags) for sub in sub_genres):
            return main_genre

    return "other"

# Add main genre to music info
logger.info("Extracting main genres...")
# Ensure 'tags' column is string type before applying split
music_info["main_genre"] = music_info["tags"].astype(str).apply(extract_main_genre)

# Merge user history with full music info to get all required columns
logger.info("Merging user history with music info...")
# Ensure track_id types match before merging
users_history['track_id'] = users_history['track_id'].astype(music_info['track_id'].dtype)

# Rename columns before merge to avoid conflicts
users_history = users_history.rename(columns={'playcount': 'playcount'})
music_info = music_info.rename(columns={'playcount': 'track_popularity'})

full_user_tracks = users_history.merge(
    music_info,
    on="track_id",
    how="inner" # Keep only interactions for tracks present in music_info
)

logger.info("Columns after merge (full_user_tracks): %s", full_user_tracks.columns.tolist()) # <-- Added log

# Check if merge resulted in empty dataframe
if full_user_tracks.empty:
    logger.error("Merge between user history and music info resulted in an empty DataFrame. Check track_ids.")
    exit()

# Create user preference profile based on genres and all audio features
logger.info("Building user preferences...")

# List of all audio features to include (ensure these columns exist in music_info)
audio_features = [
    "danceability", "energy", "key", "loudness", "mode",
    "speechiness", "acousticness", "instrumentalness",
    "liveness", "valence", "tempo", "time_signature"
]

# Verify which audio features are actually in the dataset
available_features = [feat for feat in audio_features if feat in music_info.columns]
if len(available_features) < len(audio_features):
    missing_features = set(audio_features) - set(available_features)
    logger.warning(f"Missing audio features in dataset: {missing_features}")
    logger.info(f"Using available audio features: {available_features}")

# Get genre preferences for each user (with simplified processing)
# Check if 'playcount' exists before grouping
if 'playcount' not in full_user_tracks.columns:
    logger.error(f"Critical Error: 'playcount' column is missing from full_user_tracks DataFrame after merge.")
    logger.error(f"Available columns: {full_user_tracks.columns.tolist()}")
    exit()

user_genre_counts = full_user_tracks.groupby(["user_id", "main_genre"])["playcount"].sum()
# Handle potential multi-index issues if a user has no history after merge
if user_genre_counts.empty:
    logger.warning("No genre counts generated. Check the merged data.")
    # Create an empty DataFrame with expected structure if needed later
    user_genre_prefs = pd.DataFrame(columns=['user_id', 'top_genre']).set_index('user_id')
else:
    user_genre_prefs = user_genre_counts.unstack(fill_value=0)
    # Normalize to get percentages
    row_sums = user_genre_prefs.sum(axis=1)
    # Avoid division by zero if a user has zero total playcount (shouldn't happen with synthetic data)
    row_sums[row_sums == 0] = 1
    user_genre_prefs = user_genre_prefs.div(row_sums, axis=0) * 100
    user_genre_prefs.fillna(0, inplace=True)
    # Add top genre column
    user_genre_prefs["top_genre"] = user_genre_prefs.idxmax(axis=1)
    user_genre_prefs["top_genre"] = user_genre_prefs["top_genre"].fillna("unknown")


# Get average audio features for each user (optimized implementation)
user_audio_features = pd.DataFrame(index=user_genre_prefs.index) # Use index from genre prefs
user_total_playcounts = full_user_tracks.groupby("user_id")["playcount"].sum()
user_audio_features["total_playcount"] = user_total_playcounts

# Calculate weighted averages for audio features
for feature in available_features:
    if feature in full_user_tracks.columns and not full_user_tracks[feature].isnull().all(): # Check if feature exists and has non-NA values
        # Ensure feature is numeric before calculation
        full_user_tracks[feature] = pd.to_numeric(full_user_tracks[feature], errors='coerce')
        # Handle potential NaNs introduced by coercion or already present
        full_user_tracks_feature_no_na = full_user_tracks.dropna(subset=[feature])

        if not full_user_tracks_feature_no_na.empty:
             # Calculate weighted sum for the feature
             weighted_feature_sum = (full_user_tracks_feature_no_na["playcount"] * full_user_tracks_feature_no_na[feature])\
                                     .groupby(full_user_tracks_feature_no_na["user_id"]).sum()

             # Get the corresponding total playcount for users who have non-NA values for this feature
             relevant_playcounts = user_total_playcounts.loc[weighted_feature_sum.index]
             relevant_playcounts[relevant_playcounts == 0] = 1 # Avoid division by zero

             # Calculate weighted average
             user_audio_features[f"avg_{feature}"] = weighted_feature_sum / relevant_playcounts
        else:
             logger.warning(f"Feature '{feature}' has all NaN values after coercion. Skipping average calculation.")
             user_audio_features[f"avg_{feature}"] = np.nan # Assign NaN if feature is all NaN

# Fill any remaining NaNs in avg features (e.g., for users with no valid feature data) with 0 or mean? Using 0 for now.
user_audio_features.fillna(0, inplace=True)

# Reset index and merge user features
user_genre_prefs = user_genre_prefs.reset_index()
user_audio_features = user_audio_features.reset_index()

# Check if user_ids are compatible for merge
if 'user_id' not in user_genre_prefs.columns or 'user_id' not in user_audio_features.columns:
     logger.error("Missing 'user_id' column in intermediate dataframes before merging features.")
     exit()

user_features = user_genre_prefs.merge(user_audio_features, on="user_id", how="left") # Use left merge to keep all users from genre prefs
# Fill NaNs that might result from the merge (if a user had genres but somehow no audio features calculated)
user_features.fillna({"total_playcount": 0, "top_genre": "unknown"}, inplace=True)
for feature in available_features:
     user_features.fillna({f"avg_{feature}": 0}, inplace=True)


# Define simplified demographic correlations
genre_age_map = {
    "rock": [0.15, 0.30, 0.25, 0.15, 0.10, 0.05],  # More middle-aged
    "electronic": [0.40, 0.35, 0.15, 0.05, 0.03, 0.02],  # Young adult
    "hip_hop": [0.45, 0.30, 0.15, 0.05, 0.03, 0.02],  # Youngest demographic
    "pop": [0.35, 0.30, 0.15, 0.10, 0.05, 0.05],  # Younger skew
    "classical": [0.10, 0.15, 0.15, 0.20, 0.20, 0.20],  # Oldest demographic
    "jazz": [0.05, 0.15, 0.20, 0.25, 0.20, 0.15],  # Older demographic
    "folk": [0.15, 0.20, 0.20, 0.20, 0.15, 0.10],  # Balanced but older
    "latin": [0.30, 0.30, 0.20, 0.10, 0.05, 0.05],  # Younger adult
    "rb_soul": [0.20, 0.25, 0.25, 0.15, 0.10, 0.05],  # Mixed age groups
    "country": [0.15, 0.20, 0.25, 0.20, 0.15, 0.05],  # Middle-aged skew
    "religious": [0.15, 0.20, 0.20, 0.20, 0.15, 0.10],  # Balanced distribution
    "other": [0.30, 0.30, 0.15, 0.10, 0.10, 0.05],  # Generic distribution
    "unknown": [0.30, 0.30, 0.15, 0.10, 0.10, 0.05],  # Generic distribution
}

genre_gender_map = {
    "rock": {"Male": 0.60, "Female": 0.40},
    "electronic": {"Male": 0.60, "Female": 0.40},
    "hip_hop": {"Male": 0.55, "Female": 0.45},
    "pop": {"Male": 0.35, "Female": 0.65},
    "classical": {"Male": 0.48, "Female": 0.52},
    "jazz": {"Male": 0.55, "Female": 0.45},
    "folk": {"Male": 0.45, "Female": 0.55},
    "latin": {"Male": 0.40, "Female": 0.60},
    "rb_soul": {"Male": 0.45, "Female": 0.55},
    "country": {"Male": 0.48, "Female": 0.52},
    "religious": {"Male": 0.45, "Female": 0.55},
    "other": {"Male": 0.50, "Female": 0.50},
    "unknown": {"Male": 0.50, "Female": 0.50},
}

genre_region_map = {
    "rock": {
        "Europe": 0.35,
        "North America": 0.35,
        "Latin America": 0.15,
        "Rest of World": 0.15,
    },
    "electronic": {
        "Europe": 0.40,
        "North America": 0.25,
        "Latin America": 0.15,
        "Rest of World": 0.20,
    },
    "hip_hop": {
        "Europe": 0.20,
        "North America": 0.45,
        "Latin America": 0.20,
        "Rest of World": 0.15,
    },
    "pop": {
        "Europe": 0.30,
        "North America": 0.30,
        "Latin America": 0.20,
        "Rest of World": 0.20,
    },
    "classical": {
        "Europe": 0.40,
        "North America": 0.30,
        "Latin America": 0.10,
        "Rest of World": 0.20,
    },
    "jazz": {
        "Europe": 0.30,
        "North America": 0.40,
        "Latin America": 0.15,
        "Rest of World": 0.15,
    },
    "folk": {
        "Europe": 0.35,
        "North America": 0.40,
        "Latin America": 0.10,
        "Rest of World": 0.15,
    },
    "latin": {
        "Europe": 0.15,
        "North America": 0.20,
        "Latin America": 0.55,
        "Rest of World": 0.10,
    },
    "rb_soul": {
        "Europe": 0.20,
        "North America": 0.45,
        "Latin America": 0.20,
        "Rest of World": 0.15,
    },
    "country": {
        "Europe": 0.15,
        "North America": 0.60,
        "Latin America": 0.15,
        "Rest of World": 0.10,
    },
    "religious": {
        "Europe": 0.25,
        "North America": 0.35,
        "Latin America": 0.25,
        "Rest of World": 0.15,
    },
    "other": {
        "Europe": 0.27,
        "North America": 0.28,
        "Latin America": 0.22,
        "Rest of World": 0.23,
    },
    "unknown": {
        "Europe": 0.27,
        "North America": 0.28,
        "Latin America": 0.22,
        "Rest of World": 0.23,
    },
}

# Define age groups
age_groups = [
    (18, 24),  # 31.51%
    (25, 34),  # 31.41%
    (35, 44),  # ~15%
    (45, 54),  # ~10%
    (55, 64),  # ~8%
    (65, 100),  # ~4%
]

# Use audio features to refine segmentation with all available features
def adjust_demographics(row):
    top_genre = row["top_genre"]
    
    # Get base probabilities from genre
    age_probs = genre_age_map.get(top_genre, genre_age_map["other"]).copy()
    gender_probs = genre_gender_map.get(top_genre, genre_gender_map["other"]).copy()
    region_probs = genre_region_map.get(top_genre, genre_region_map["other"]).copy()

    # Adjust based on audio features if available
    if "avg_energy" in row and pd.notna(row["avg_energy"]) and row["avg_energy"] > 0.7:
         for i in range(2):  # Boost first two age groups
             age_probs[i] *= 1.2

    if "avg_valence" in row and pd.notna(row["avg_valence"]) and row["avg_valence"] > 0.6:
         gender_probs["Female"] *= 1.1

    if "avg_danceability" in row and pd.notna(row["avg_danceability"]) and row["avg_danceability"] > 0.7:
         region_probs["Latin America"] *= 1.2
         region_probs["Europe"] *= 1.1
        
    if "avg_acousticness" in row and pd.notna(row["avg_acousticness"]) and row["avg_acousticness"] > 0.7:
         for i in range(3, 6):  # Boost older age groups
             age_probs[i] *= 1.15
             
    if "avg_speechiness" in row and pd.notna(row["avg_speechiness"]) and row["avg_speechiness"] > 0.3:
         for i in range(2):  # Boost youngest age groups
             age_probs[i] *= 1.2
         region_probs["North America"] *= 1.1

    # Normalize probabilities efficiently
    age_probs = np.array(age_probs)
    age_probs = age_probs / age_probs.sum()

    gender_values = np.array(list(gender_probs.values()))
    gender_values = gender_values / gender_values.sum()
    gender_probs = {k: gender_values[i] for i, k in enumerate(gender_probs.keys())}

    region_values = np.array(list(region_probs.values()))
    region_values = region_values / region_values.sum()
    region_probs = {k: region_values[i] for i, k in enumerate(region_probs.keys())}

    return age_probs, gender_probs, region_probs

# Create country lists by region
countries_by_region = {
    "Europe": ["UK", "Germany", "France", "Spain", "Italy", "Sweden", "Netherlands", "Poland"],
    "North America": ["USA", "Canada"],
    "Latin America": ["Brazil", "Mexico", "Argentina", "Colombia", "Chile"],
    "Rest of World": ["Japan", "Australia", "India", "South Korea", "South Africa", "UAE"],
}

# Generate demographics
logger.info("Generating demographics...")
demographics = []

# Check if user_features is empty before iterating
if user_features.empty:
    logger.warning("User features DataFrame is empty. No demographics will be generated.")
else:
    for _, row in user_features.iterrows():
        user_id = row["user_id"]

        # Get adjusted demographic probabilities
        age_probs, gender_probs, region_probs = adjust_demographics(row)

        # Assign age
        age_group_idx = np.random.choice(len(age_groups), p=age_probs)
        age_range = age_groups[age_group_idx]
        age = np.random.randint(age_range[0], age_range[1] + 1)

        # Assign gender
        gender = np.random.choice(list(gender_probs.keys()), p=list(gender_probs.values()))

        # Assign region
        region = np.random.choice(list(region_probs.keys()), p=list(region_probs.values()))

        # Assign country
        country = np.random.choice(countries_by_region[region])

        # Assign listening hours based on age and gender
        base_hours = 0
        if age < 25:
            base_hours = 25 + np.random.normal(5, 3)
        elif age < 35:
            base_hours = 20 + np.random.normal(4, 3)
        elif age < 45:
            base_hours = 15 + np.random.normal(5, 2)
        else:
            base_hours = 10 + np.random.normal(5, 2)

        # Gen Z women tend to listen more
        if age < 25 and gender == "Female":
            base_hours *= 1.2

        monthly_hours = max(5, min(60, base_hours))  # Cap between 5 and 60 hours

        # Calculate genre diversity (number of genres with >5% listening)
        genre_cols = [col for col in user_genre_prefs.columns if col not in ["user_id", "top_genre"]] # Adjusted column exclusion
        # Ensure row[col] access is safe if user_features structure changed
        diversity_score = sum(1 for col in genre_cols if col in row and pd.notna(row[col]) and row[col] > 5)


        # Create demographic record
        demo_data = {
            "user_id": user_id,
            "age": age,
            "gender": gender,
            "region": region,
            "country": country,
            "monthly_hours": monthly_hours,
            "top_genre": row["top_genre"],
            "genre_diversity": diversity_score,
        }
        
        # Add all audio features
        for feature in available_features:
            feature_col = f"avg_{feature}"
            if feature_col in row:
                demo_data[feature_col] = row[feature_col]
        
        demographics.append(demo_data)

# Create final demographics dataframe
user_demographics = pd.DataFrame(demographics)

# Ensure output directories exist
os.makedirs(os.path.dirname(OUTPUT_COMPLETE_DATASET), exist_ok=True)
os.makedirs(os.path.dirname(OUTPUT_USER_DEMOGRAPHICS), exist_ok=True)


# Create the final dataset with all required columns by merging with full_user_tracks
logger.info("Creating final merged dataset with all required columns...")

# Check if user_demographics is empty
if user_demographics.empty:
    logger.warning("User demographics DataFrame is empty. Cannot create final dataset.")
    final_dataset = pd.DataFrame() # Create empty df to avoid errors later
else:
    # Aggregate playcount by track_id and user_id from the original merged data
    # This ensures we have one row per user-track interaction for the final merge
    playcount_agg = full_user_tracks.groupby(['user_id', 'track_id'])['playcount'].sum().reset_index()

    # Get unique tracks data - Select columns carefully from music_info
    # Use column names from music_info ('song_name', 'artist_name' etc.)
    track_columns = ['track_id', 'song_name', 'artist_name', 'main_genre', 'year', 'duration_ms'] + available_features
    # Add other potentially useful columns if they exist
    for col in ['spotify_preview_url', 'spotify_id', 'tags', 'genre']: # 'genre' might be original, 'main_genre' is derived
        if col in music_info.columns:
            track_columns.append(col)
    track_columns = list(set(track_columns)) # Ensure unique columns

    # Check if all selected track_columns exist in music_info
    existing_track_columns = [col for col in track_columns if col in music_info.columns]
    missing_track_cols = set(track_columns) - set(existing_track_columns)
    if missing_track_cols:
        logger.warning(f"Following track columns specified but not found in music_info: {missing_track_cols}")

    unique_tracks = music_info[existing_track_columns].drop_duplicates('track_id')

    # Merge aggregated playcounts with unique track info
    final_dataset_base = playcount_agg.merge(unique_tracks, on='track_id', how='left')

    # Now merge with user_demographics
    final_dataset = final_dataset_base.merge(
        user_demographics,
        on='user_id',
        how='inner', # Keep only users for whom demographics were generated
        suffixes=('', '_user') # Suffix to distinguish user-level features if needed
    )

# Save the final dataset with all required columns
logger.info(f"Final dataset shape: {final_dataset.shape}")
if not final_dataset.empty:
    logger.info(f"Final dataset columns: {final_dataset.columns.tolist()}")
    logger.info(f"Saving final dataset to {OUTPUT_COMPLETE_DATASET}...")
    final_dataset.to_csv(OUTPUT_COMPLETE_DATASET, index=False)
else:
    logger.warning("Final dataset is empty. Skipping save.")


# Also save the user demographics separately for reference
logger.info(f"Saving demographic file to {OUTPUT_USER_DEMOGRAPHICS}...")
if not user_demographics.empty:
    user_demographics.to_csv(OUTPUT_USER_DEMOGRAPHICS, index=False)
else:
    logger.warning("User demographics DataFrame is empty. Skipping save.")


# Display summary statistics (only if data exists)
if not user_demographics.empty:
    logger.info("\nDemographic Distribution Overview:")
    logger.info(f"Age: Mean={user_demographics['age'].mean():.1f}, Median={user_demographics['age'].median()}")

    logger.info("\nAge Groups:")
    age_groups_count = user_demographics['age'].value_counts(bins=[18, 24, 34, 44, 54, 64, 100]).sort_index()
    logger.info(f"{age_groups_count}")

    logger.info("\nGender Distribution:")
    gender_dist = user_demographics["gender"].value_counts(normalize=True) * 100
    logger.info(f"{gender_dist}")

    logger.info("\nRegion Distribution:")
    region_dist = user_demographics["region"].value_counts(normalize=True) * 100
    logger.info(f"{region_dist}")

    logger.info("\nTop Genres:")
    top_genres = user_demographics["top_genre"].value_counts().head(10)
    logger.info(f"{top_genres}")

# Display year distribution of songs used (from the loaded music_info)
logger.info("\nYear Distribution of Songs Used:")
year_desc = music_info["year"].describe()
logger.info(f"{year_desc}")

# Display audio features summary (only if data exists)
if not user_demographics.empty:
    logger.info("\nAudio Features Summary (User Averages):")
    for feature in available_features:
        if f"avg_{feature}" in user_demographics.columns:
            logger.info(f"{feature}: Mean={user_demographics[f'avg_{feature}'].mean():.4f}")

logger.info("\nProcess complete!")

# Calculate additional quality metrics for final dataset (only if data exists)
if not final_dataset.empty:
    logger.info("\nPerforming quality checks on final dataset...")

    # Check for missing values in the final dataset
    missing_values_final = final_dataset.isnull().sum()
    logger.info(f"Missing values in final dataset:\n{missing_values_final[missing_values_final > 0]}")

    # Check for numerical outliers in key columns
    for feature in available_features:
        if feature in final_dataset.columns:
            q1 = final_dataset[feature].quantile(0.25)
            q3 = final_dataset[feature].quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            outliers = final_dataset[(final_dataset[feature] < lower_bound) | (final_dataset[feature] > upper_bound)]
            if len(outliers) > 0:
                outlier_pct = (len(outliers) / len(final_dataset)) * 100
                logger.info(f"Outliers in {feature}: {len(outliers)} records ({outlier_pct:.2f}%)")

    # Check demographic distribution matches our expected probabilities (only if data exists)
    if not user_demographics.empty:
        logger.info("\nVerifying demographic distributions match expected probabilities...")

        # Age distribution check
        age_group_data = []
        for i, (min_age, max_age) in enumerate(age_groups):
            group_name = f"{min_age}-{max_age}"
            actual_count = len(user_demographics[(user_demographics['age'] >= min_age) & (user_demographics['age'] <= max_age)])
            actual_pct = (actual_count / len(user_demographics)) * 100
            
            # Calculate expected percentages (average across all genres)
            expected_pcts = [genre_age_map[genre][i] for genre in genre_age_map.keys()]
            expected_pct = np.mean(expected_pcts) * 100
            
            age_group_data.append({
                'group': group_name,
                'actual_pct': actual_pct,
                'expected_pct': expected_pct,
                'difference': actual_pct - expected_pct
            })

        age_distribution_check = pd.DataFrame(age_group_data)
        logger.info(f"Age distribution check:\n{age_distribution_check}")

        # Gender distribution check
        gender_actual = user_demographics['gender'].value_counts(normalize=True) * 100
        gender_expected = {gender: np.mean([probs[gender] for probs in genre_gender_map.values()]) * 100 
                          for gender in ['Male', 'Female']}
        logger.info(f"Gender distribution - Actual vs Expected:")
        for gender in gender_expected:
            logger.info(f"{gender}: Actual {gender_actual.get(gender, 0):.2f}% vs Expected {gender_expected[gender]:.2f}%")

    # Additional playcount analysis
    logger.info("\nPlaycount Statistics:")
    logger.info(f"Mean playcount per user-track: {final_dataset['playcount'].mean():.2f}")
    logger.info(f"Median playcount per user-track: {final_dataset['playcount'].median():.2f}")
    logger.info(f"Min playcount: {final_dataset['playcount'].min()}")
    logger.info(f"Max playcount: {final_dataset['playcount'].max()}")

    # Check for users with unusually high playcounts
    user_total_plays = final_dataset.groupby('user_id')['playcount'].sum()
    high_playcount_users = user_total_plays[user_total_plays > user_total_plays.quantile(0.95)]
    if len(high_playcount_users) > 0:
        logger.info(f"Users with unusually high playcounts (top 5%):")
        logger.info(f"{high_playcount_users.sort_values(ascending=False).head()}")

    logger.info("Quality checks complete!")
else:
    logger.warning("Final dataset is empty. Skipping quality checks.")
