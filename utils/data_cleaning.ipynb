{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../data/music_data.csv\"\n",
    "data = pd.read_csv(filepath)\n",
    "df = data.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/martinson/Lhydra_rs/data/raw_data/enriched_synthetic_data.csv\"\n",
    "test_df = pd.read_csv(filepath)\n",
    "data = test_df.copy()\n",
    "data.columns\n",
    "# check for missing release_dates\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"release_date\"].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. correct encoding of user ids with hashing\n",
    "# 2. correct encoding of music ids with hashing\n",
    "# 3. verify number of unique users and songs match by shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"music\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assuming you have a DataFrame 'df' with columns 'music' and 'user_id'\n",
    "# songs = df['music'].unique()\n",
    "# users = df['user_id'].unique()\n",
    "\n",
    "# # Create a DataFrame to store playcounts\n",
    "# playcounts_df = pd.DataFrame(index=users, columns=songs)\n",
    "\n",
    "# # Generate playcounts using Poisson distribution in a vectorized manner\n",
    "# lambda_value = 5  # Desired average playcount\n",
    "# playcounts = np.random.poisson(lam=lambda_value, size=(len(users), len(songs)))\n",
    "\n",
    "# # Assign playcounts to the DataFrame\n",
    "# playcounts_df[:] = playcounts\n",
    "\n",
    "# # Display the playcounts DataFrame\n",
    "# playcounts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the playcounts DataFrame to long format\n",
    "playcounts_long = playcounts_df.reset_index().melt(\n",
    "    id_vars=\"index\", var_name=\"music\", value_name=\"playcount\"\n",
    ")\n",
    "playcounts_long.rename(columns={\"index\": \"user_id\"}, inplace=True)\n",
    "\n",
    "# Merge the playcounts with the existing DataFrame\n",
    "df_updated = df.merge(playcounts_long, on=[\"user_id\", \"music\"], how=\"left\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of playcount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_updated[\"playcount\"], kde=True, bins=30)\n",
    "plt.title(\"Distribution of Playcount\")\n",
    "plt.xlabel(\"Playcount\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.drop(columns=[\"plays\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_updated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check for unique music_ids\n",
    "unique_music_ids = df[\"music_id\"].nunique()\n",
    "total_rows = len(df)\n",
    "if unique_music_ids != total_rows:\n",
    "    logging.warning(\n",
    "        f\"Mismatch in music_ids: Expected {total_rows} unique IDs but found {unique_music_ids}\"\n",
    "    )\n",
    "    # Assuming we need to rehash music_ids\n",
    "    df[\"music_id\"] = df[\"music\"].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())\n",
    "    logging.info(\"Rehashed music_ids to ensure uniqueness.\")\n",
    "else:\n",
    "    logging.info(\"All music_ids are unique.\")\n",
    "\n",
    "# Debug: Show the updated data after rehashing music_ids (if needed)\n",
    "print(\"\\nData after music_id rehashing:\")\n",
    "df.head()\n",
    "\n",
    "# Step 2: Restructure user_ids\n",
    "unique_users = df[\"user_id\"].unique()\n",
    "user_id_mapping = {\n",
    "    old_id: new_id for new_id, old_id in enumerate(unique_users, start=1)\n",
    "}\n",
    "\n",
    "# Apply the mapping to create new user_ids\n",
    "df[\"new_user_id\"] = df[\"user_id\"].map(user_id_mapping)\n",
    "\n",
    "# Debug: Show the updated data after restructuring user_ids\n",
    "print(\"\\nData after user_id restructuration:\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Optional Step 3: Hash new_user_ids for robustness\n",
    "def hash_user_id(user_id):\n",
    "    return hashlib.sha256(str(user_id).encode()).hexdigest()\n",
    "\n",
    "\n",
    "df[\"hashed_user_id\"] = df[\"new_user_id\"].apply(hash_user_id)\n",
    "\n",
    "# Debug: Show the final data after hashing user_ids (if needed)\n",
    "print(\"\\nFinal data after hashing user_ids:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"music_id\"].nunique(), df[\"music\"].nunique()\n",
    "print(\n",
    "    f'Number of unique music ids: {df[\"music_id\"].nunique()}\\nNumber of unique songs: {df[\"music\"].nunique()}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"user_id\", \"new_user_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename hased_user_id to user_id\n",
    "df.rename(columns={\"hashed_user_id\": \"user_id\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying genre extraction with spotify api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "missing_genres = df[df[\"genre\"].isin([\"Unknown\", \"Other\", np.nan])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import configparser\n",
    "import time\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Read and clean credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "client_id = config[\"SPOTIFY\"][\"CLIENT_ID\"].strip(\"'\")  # Remove quotes\n",
    "client_secret = config[\"SPOTIFY\"][\"CLIENT_SECRET\"].strip(\"'\")\n",
    "\n",
    "\n",
    "def create_spotify_client(max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            client_credentials_manager = SpotifyClientCredentials(\n",
    "                client_id=client_id, client_secret=client_secret\n",
    "            )\n",
    "            return spotipy.Spotify(\n",
    "                client_credentials_manager=client_credentials_manager\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise Exception(f\"Failed to authenticate with Spotify: {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "def get_genre_from_spotify(artist_name, track_name, sp, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            results = sp.search(\n",
    "                q=f\"artist:{artist_name} track:{track_name}\", type=\"track\", limit=1\n",
    "            )\n",
    "            if not results[\"tracks\"][\"items\"]:\n",
    "                return None\n",
    "\n",
    "            track = results[\"tracks\"][\"items\"][0]\n",
    "            artist_id = track[\"artists\"][0][\"id\"]\n",
    "            artist = sp.artist(artist_id)\n",
    "            return artist[\"genres\"][0] if artist[\"genres\"] else None\n",
    "\n",
    "        except RequestException as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Error fetching data from Spotify: {e}\")\n",
    "                return None\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "# Initialize Spotify client\n",
    "sp = create_spotify_client()\n",
    "\n",
    "# Process missing genres with retry logic\n",
    "missing_genres[\"genre_spotify\"] = missing_genres.apply(\n",
    "    lambda row: get_genre_from_spotify(row[\"artist_name\"], row[\"music\"], sp), axis=1\n",
    ")\n",
    "\n",
    "# Merge results\n",
    "df = df.merge(\n",
    "    missing_genres[[\"music\", \"artist_name\", \"genre_spotify\"]],\n",
    "    on=[\"music\", \"artist_name\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "df[\"genre\"] = df[\"genre\"].fillna(df[\"genre_spotify\"])\n",
    "df = df.drop(\"genre_spotify\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    missing_genres[[\"music\", \"artist_name\", \"genre_spotify\"]],\n",
    "    on=[\"music\", \"artist_name\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "# df['genre'] = df['genre'].fillna(df['genre_spotify'])\n",
    "# df = df.drop('genre_spotify', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=['genre_spotify_x','genre_spotify_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create missing genres mask\n",
    "missing_mask = df[\"genre\"].isin([\"Unknown\", \"Other\"]) | df[\"genre\"].isna()\n",
    "\n",
    "# 2. Get subset for Spotify API calls\n",
    "missing_df = df[missing_mask].copy()\n",
    "\n",
    "# 3. Get Spotify genres\n",
    "# missing_df['genre_spotify'] = missing_df.apply(\n",
    "#     lambda row: get_genre_from_spotify(row['artist_name'], row['music']),\n",
    "#     axis=1\n",
    "# )\n",
    "missing_df = missing_genres.copy()\n",
    "\n",
    "# 4. Create update mapping\n",
    "genre_updates = missing_df[[\"music\", \"artist_name\", \"genre_spotify\"]].dropna()\n",
    "\n",
    "# 5. Update original DataFrame using merge\n",
    "# df = df.merge(\n",
    "#     genre_updates,\n",
    "#     on=['music', 'artist_name'],\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "# 6. Update genre column with new values where applicable\n",
    "df[\"genre\"] = df.apply(\n",
    "    lambda row: (\n",
    "        row[\"genre_spotify\"]\n",
    "        if pd.isnull(row[\"genre\"]) or row[\"genre\"] in [\"Unknown\", \"Other\"]\n",
    "        else row[\"genre\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# 7. Clean up\n",
    "# df = df.drop('genre_spotify', axis=1)\n",
    "\n",
    "# 8. Verify results\n",
    "print(f\"Unique genres: {df['genre'].nunique()}\")\n",
    "print(f\"Missing genres: {df['genre'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/synthetic_cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing genre with \"Unknown\"\n",
    "df[\"genre\"] = df[\"genre\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = df[\"genre\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of genre\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y=df[\"genre\"], order=df[\"genre\"].value_counts().index)\n",
    "plt.title(\"Distribution of Genre\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_genres['genre_spotify'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# List of genre names\n",
    "\n",
    "# Vectorize the genre names using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(unique_genres)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5)\n",
    "clustering.fit(X.toarray())\n",
    "\n",
    "# Get the cluster labels for each genre\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Group the unique_genres.tolist()) by their cluster labels\n",
    "from collections import defaultdict\n",
    "\n",
    "genre_groups = defaultdict(list)\n",
    "for genre, label in zip((unique_genres), labels):\n",
    "    genre_groups[label].append(genre)\n",
    "\n",
    "# Print the genre groups\n",
    "for label, group in genre_groups.items():\n",
    "    print(f\"Cluster {label}: {', '.join(group)}\")\n",
    "\n",
    "# how many clusters\n",
    "n_clusters = len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def cluster_genres(genres, n_clusters=20, plot=True):\n",
    "    \"\"\"\n",
    "    Cluster genres and visualize relationships\n",
    "    \"\"\"\n",
    "    # Vectorize genres\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(genres)\n",
    "\n",
    "    # Cluster\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = clustering.fit_predict(X.toarray())\n",
    "\n",
    "    # Group genres\n",
    "    genre_groups = defaultdict(list)\n",
    "    for genre, label in zip(genres, labels):\n",
    "        genre_groups[label].append(genre)\n",
    "\n",
    "    if plot:\n",
    "        # Create graph\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Add nodes and edges\n",
    "        for label, group in genre_groups.items():\n",
    "            for genre in group:\n",
    "                G.add_node(genre, cluster=label)\n",
    "                for other_genre in group:\n",
    "                    if genre != other_genre:\n",
    "                        G.add_edge(genre, other_genre)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        pos = nx.spring_layout(G)\n",
    "        colors = [G.nodes[node][\"cluster\"] for node in G.nodes()]\n",
    "\n",
    "        nx.draw_networkx(\n",
    "            G,\n",
    "            pos,\n",
    "            node_color=colors,\n",
    "            node_size=1000,\n",
    "            font_size=8,\n",
    "            cmap=plt.cm.tab20,\n",
    "            with_labels=True,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Genre Clusters (n={n_clusters})\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return genre_groups\n",
    "\n",
    "\n",
    "# Usage\n",
    "unique_genres = df[\"genre\"].unique()\n",
    "clusters = cluster_genres(unique_genres, n_clusters=20)\n",
    "\n",
    "# Print clusters\n",
    "for label, genres in clusters.items():\n",
    "    print(f\"\\nCluster {label}:\")\n",
    "    print(\", \".join(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import umap\n",
    "import hdbscan\n",
    "import plotly.express as px\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def cluster_genres_advanced(genres, n_neighbors=15, min_cluster_size=5):\n",
    "    \"\"\"\n",
    "    Advanced genre clustering using Word2Vec + UMAP + HDBSCAN\n",
    "    \"\"\"\n",
    "    # 1. Prepare text data\n",
    "    genres = [str(g).lower() for g in genres if str(g) != \"nan\"]\n",
    "\n",
    "    # 2. Get embeddings using SentenceTransformer\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(genres)\n",
    "\n",
    "    # 3. Dimensionality reduction\n",
    "    umap_reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, min_dist=0.0, n_components=2, random_state=42\n",
    "    )\n",
    "    umap_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "    # 4. Clustering\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size, min_samples=1, prediction_data=True\n",
    "    )\n",
    "    cluster_labels = clusterer.fit_predict(umap_embeddings)\n",
    "\n",
    "    # 5. Create DataFrame for visualization\n",
    "    viz_df = pd.DataFrame(\n",
    "        {\n",
    "            \"genre\": genres,\n",
    "            \"x\": umap_embeddings[:, 0],\n",
    "            \"y\": umap_embeddings[:, 1],\n",
    "            \"cluster\": cluster_labels,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 6. Interactive visualization\n",
    "    fig = px.scatter(\n",
    "        viz_df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"cluster\",\n",
    "        hover_data=[\"genre\"],\n",
    "        title=\"Genre Clusters\",\n",
    "        template=\"plotly_dark\",\n",
    "    )\n",
    "\n",
    "    # 7. Group results\n",
    "    clusters = {}\n",
    "    for label in set(cluster_labels):\n",
    "        if label != -1:  # Exclude noise points\n",
    "            clusters[f\"Cluster_{label}\"] = viz_df[viz_df[\"cluster\"] == label][\n",
    "                \"genre\"\n",
    "            ].tolist()\n",
    "\n",
    "    return clusters, fig\n",
    "\n",
    "\n",
    "# Usage\n",
    "unique_genres = df[\"genre\"].dropna().unique()\n",
    "clusters, fig = cluster_genres_advanced(unique_genres)\n",
    "\n",
    "# Display interactive plot\n",
    "fig.show()\n",
    "\n",
    "# Print clusters\n",
    "for name, genres in clusters.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\", \".join(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "df[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class GenreClustering:\n",
    "    MAIN_GENRES = {\n",
    "        \"rock\": [\"rock\", \"metal\", \"punk\", \"alternative\", \"grunge\", \"indie\"],\n",
    "        \"electronic\": [\"electronic\", \"dance\", \"edm\", \"house\", \"techno\", \"dubstep\"],\n",
    "        \"hip_hop\": [\"hip-hop\", \"rap\", \"trap\", \"drill\"],\n",
    "        \"pop\": [\"pop\", \"teen pop\", \"dance pop\", \"k-pop\"],\n",
    "        \"classical\": [\"classical\", \"baroque\", \"orchestra\"],\n",
    "        \"jazz\": [\"jazz\", \"bebop\", \"fusion\"],\n",
    "        \"folk\": [\"folk\", \"acoustic\", \"singer-songwriter\"],\n",
    "        \"rb_soul\": [\"rnb\", \"r&b\", \"soul\", \"motown\"],\n",
    "        \"country\": [\"country\", \"bluegrass\", \"americana\"],\n",
    "        \"world\": [\"latin\", \"reggae\", \"afrobeat\", \"world\"],\n",
    "        \"religious\": [\"christian\", \"gospel\", \"spiritual\", \"worship\"],\n",
    "    }\n",
    "\n",
    "    def __init__(self, genres_series):\n",
    "        self.genres = genres_series\n",
    "        self.genre_map = self._create_genre_map()\n",
    "\n",
    "    def _create_genre_map(self):\n",
    "        \"\"\"Create mapping of subgenres to main genres\"\"\"\n",
    "        genre_map = {}\n",
    "        for main_genre, subgenres in self.MAIN_GENRES.items():\n",
    "            for subgenre in subgenres:\n",
    "                genre_map[subgenre] = main_genre\n",
    "        return genre_map\n",
    "\n",
    "    def classify_genre(self, genre):\n",
    "        \"\"\"Map a genre to its main category\"\"\"\n",
    "        genre = str(genre).lower()\n",
    "        for key_term, main_genre in self.genre_map.items():\n",
    "            if key_term in genre:\n",
    "                return main_genre\n",
    "        return \"other\"\n",
    "\n",
    "    def cluster_genres(self):\n",
    "        \"\"\"Group genres into main categories\"\"\"\n",
    "        clustered = defaultdict(list)\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        for genre, count in self.genres.items():\n",
    "            main_genre = self.classify_genre(genre)\n",
    "            clustered[main_genre].append((genre, count))\n",
    "            counts[main_genre] += count\n",
    "\n",
    "        return clustered, counts\n",
    "\n",
    "\n",
    "# Usage\n",
    "genres_series = pd.Series(\n",
    "    {\n",
    "        genre: count\n",
    "        for genre, count in zip(\n",
    "            df[\"genre\"].value_counts().index, df[\"genre\"].value_counts().values\n",
    "        )\n",
    "    }\n",
    ")\n",
    "clusterer = GenreClustering(genres_series)\n",
    "clusters, counts = clusterer.cluster_genres()\n",
    "\n",
    "# Print results\n",
    "for main_genre, subgenres in clusters.items():\n",
    "    print(f\"\\n## {main_genre.upper()} (Total: {counts[main_genre]})\")\n",
    "    for subgenre, count in sorted(subgenres, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"- {subgenre}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GenreProcessor:\n",
    "    GENRE_HIERARCHY = {\n",
    "        \"rock\": [\"rock\", \"metal\", \"punk\", \"alternative\", \"grunge\", \"indie\"],\n",
    "        \"electronic\": [\"electronic\", \"dance\", \"edm\", \"house\", \"techno\", \"dubstep\"],\n",
    "        \"hip_hop\": [\"hip-hop\", \"rap\", \"trap\", \"drill\"],\n",
    "        \"pop\": [\"pop\", \"teen pop\", \"dance pop\", \"k-pop\"],\n",
    "        \"classical\": [\"classical\", \"baroque\", \"orchestra\"],\n",
    "        \"jazz\": [\"jazz\", \"bebop\", \"fusion\"],\n",
    "        \"folk\": [\"folk\", \"acoustic\", \"singer-songwriter\"],\n",
    "        \"rb_soul\": [\"rnb\", \"r&b\", \"soul\", \"motown\"],\n",
    "        \"country\": [\"country\", \"bluegrass\", \"americana\"],\n",
    "        \"world\": [\"latin\", \"reggae\", \"afrobeat\", \"world\"],\n",
    "        \"religious\": [\"christian\", \"gospel\", \"spiritual\", \"worship\"],\n",
    "    }\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.genre_map = self._create_full_genre_map()\n",
    "\n",
    "    def _create_full_genre_map(self):\n",
    "        \"\"\"Create comprehensive genre mapping\"\"\"\n",
    "        genre_map = {}\n",
    "        for main_genre, subgenres in self.GENRE_HIERARCHY.items():\n",
    "            for subgenre in subgenres:\n",
    "                genre_map[subgenre] = main_genre\n",
    "        return genre_map\n",
    "\n",
    "    def get_main_genre(self, genre):\n",
    "        \"\"\"Map any genre to main category using fuzzy matching\"\"\"\n",
    "        if pd.isna(genre):\n",
    "            return \"unknown\"\n",
    "        genre = str(genre).lower()\n",
    "        matches = process.extractBests(genre, self.genre_map.keys(), score_cutoff=60)\n",
    "        if matches:\n",
    "            return self.genre_map[matches[0][0]]\n",
    "        return \"other\"\n",
    "\n",
    "    def process_genres(self):\n",
    "        \"\"\"Add main genre column to DataFrame\"\"\"\n",
    "        self.df[\"main_genre\"] = self.df[\"genre\"].apply(self.get_main_genre)\n",
    "        return self.df\n",
    "\n",
    "    def match_user_genre(self, user_input):\n",
    "        \"\"\"Match user input genre to known genres\"\"\"\n",
    "        user_genre = str(user_input).lower()\n",
    "        main_genre = self.get_main_genre(user_genre)\n",
    "        return {\n",
    "            \"input_genre\": user_genre,\n",
    "            \"main_genre\": main_genre,\n",
    "            \"similar_genres\": [k for k, v in self.genre_map.items() if v == main_genre],\n",
    "        }\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "processor = GenreProcessor(df)\n",
    "df_processed = processor.process_genres()\n",
    "\n",
    "# Example of handling user input\n",
    "user_genre = \"indie rock\"\n",
    "genre_info = processor.match_user_genre(user_genre)\n",
    "print(f\"Main Genre: {genre_info['main_genre']}\")\n",
    "print(f\"Similar Genres: {', '.join(genre_info['similar_genres'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed[\"main_genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_processed.copy()\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import hashlib\n",
    "# import logging\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "# df = df_processed.copy()\n",
    "\n",
    "# # Step 1: Check for unique music_ids\n",
    "# unique_music_ids = df[\"music_id\"].nunique()\n",
    "# total_rows = len(df)\n",
    "# if unique_music_ids != total_rows:\n",
    "#     logging.warning(\n",
    "#         f\"Mismatch in music_ids: Expected {total_rows} unique IDs but found {unique_music_ids}\"\n",
    "#     )\n",
    "#     # Assuming we need to rehash music_ids\n",
    "#     df[\"music_id\"] = df[\"music\"].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())\n",
    "#     logging.info(\"Rehashed music_ids to ensure uniqueness.\")\n",
    "# else:\n",
    "#     logging.info(\"All music_ids are unique.\")\n",
    "\n",
    "# # Debug: Show the updated data after rehashing music_ids (if needed)\n",
    "# print(\"\\nData after music_id rehashing:\")\n",
    "# print(df.head())\n",
    "\n",
    "# # Step 2: Restructure user_ids\n",
    "# # unique_users = df[\"user_id\"].unique()\n",
    "# # user_id_mapping = {\n",
    "# #     old_id: new_id for new_id, old_id in enumerate(unique_users, start=1)\n",
    "# # }\n",
    "\n",
    "# # # Apply the mapping to create new user_ids\n",
    "# # df[\"new_user_id\"] = df[\"user_id\"].map(user_id_mapping)\n",
    "\n",
    "# # # Debug: Show the updated data after restructuring user_ids\n",
    "# # print(\"\\nData after user_id restructuration:\")\n",
    "# # print(df.head())\n",
    "\n",
    "\n",
    "# # Optional Step 3: Hash new_user_ids for robustness\n",
    "# # def hash_user_id(user_id):\n",
    "# #     return hashlib.sha256(str(user_id).encode()).hexdigest()\n",
    "\n",
    "\n",
    "# # df[\"hashed_user_id\"] = df[\"new_user_id\"].apply(hash_user_id)\n",
    "\n",
    "# # # Debug: Show the final data after hashing user_ids (if needed)\n",
    "# # print(\"\\nFinal data after hashing user_ids:\")\n",
    "# # print(df.head())\n",
    "\n",
    "# # Step 4: Check for class imbalances\n",
    "# categorical_columns = [\"gender\", \"main_genre\", \"explicit\"]\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     sns.countplot(x=column, data=df)\n",
    "#     plt.title(f\"Distribution of {column}\")\n",
    "#     plt.show()\n",
    "\n",
    "# # Step 5: Handle missing values\n",
    "# # Check for missing values in each column\n",
    "# missing_values = df.isnull().sum()\n",
    "# print(\"\\nMissing Values in Each Column:\")\n",
    "# print(missing_values)\n",
    "\n",
    "# # Handle missing values in 'genre' and 'featured_artists'\n",
    "# # Example: Fill 'Unknown' with 'Other'\n",
    "# # df[\"genre\"].replace(\"Unknown\", \"Other\", inplace=True)\n",
    "# df[\"featured_artists\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "# # Debug: Show the updated data after handling missing values\n",
    "# print(\"\\nData after handling missing values:\")\n",
    "# print(df.head())\n",
    "\n",
    "# # Step 6: Data Type Conversion\n",
    "# # Convert 'release_date' to datetime\n",
    "# df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
    "\n",
    "# # Debug: Show the updated data types\n",
    "# print(\"\\nData Types after conversion:\")\n",
    "# print(df.dtypes)\n",
    "\n",
    "# # Step 7: Data Consistency\n",
    "# # Check for duplicate entries\n",
    "# duplicates = df.duplicated().sum()\n",
    "# print(f\"\\nNumber of duplicate entries: {duplicates}\")\n",
    "\n",
    "# # Check for unexpected values in 'release_year'\n",
    "# if (df[\"release_year\"] < 1900).any() or (\n",
    "#     df[\"release_year\"] > datetime.now().year\n",
    "# ).any():\n",
    "#     logging.warning(\"Unexpected values found in 'release_year'.\")\n",
    "\n",
    "# # Additional feature engineering can be done here if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"featured_artists\"].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"genre_spotify\", \"genre\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/synthetic_cleaned_data_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/synthetic_cleaned_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to convert to integer\n",
    "todo = [\"playcount\", \"age\"]\n",
    "\n",
    "\n",
    "def convert_to_int(columns, dataframe):\n",
    "    # Filter columns that exist in the DataFrame\n",
    "    existing_columns = [col for col in columns if col in dataframe.columns]\n",
    "\n",
    "    for col in existing_columns:\n",
    "        try:\n",
    "            dataframe[col] = dataframe[col].astype(int)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error converting column {col} to integer: {e}\")\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Convert specified columns to integer\n",
    "df = convert_to_int(todo, df)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already defined and contains the cleaned data from previous steps\n",
    "\n",
    "# Step 1: Descriptive Statistics\n",
    "numeric_columns = df.select_dtypes(\n",
    "    include=[\n",
    "        np.number,\n",
    "    ]\n",
    ").columns\n",
    "descriptive_stats = df[numeric_columns].describe()\n",
    "print(\"\\nDescriptive Statistics for Numeric Columns:\")\n",
    "print(descriptive_stats)\n",
    "\n",
    "# Step 2: Box Plots\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Calculate the number of rows needed\n",
    "num_columns = len(numeric_columns)\n",
    "num_rows = (num_columns + 3) // 4  # 4 columns per row\n",
    "\n",
    "# Plotting box plots for each numeric column\n",
    "for i, column in enumerate(numeric_columns, 1):\n",
    "    plt.subplot(num_rows, 4, i)\n",
    "    sns.boxplot(y=df[column])\n",
    "    plt.title(f\"Boxplot of {column}\")\n",
    "    plt.ylabel(column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Correlation Analysis\n",
    "correlation_matrix = df[numeric_columns].corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    cbar_kws={\"label\": \"Correlation Coefficient\"},\n",
    ")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 4: Outlier Detection Using IQR\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "\n",
    "# Detecting outliers for each numeric column\n",
    "outliers = {}\n",
    "for column in numeric_columns:\n",
    "    outliers[column] = detect_outliers_iqr(df, column)\n",
    "    if not outliers[column].empty:\n",
    "        logging.warning(f\"Outliers detected in {column}:\")\n",
    "        print(outliers[column])\n",
    "\n",
    "\n",
    "# Step 5: Handle Outliers (Optional)\n",
    "# For demonstration, let's handle outliers by capping them at the 1.5*IQR range\n",
    "def cap_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Capping outliers for each numeric column\n",
    "for column in numeric_columns:\n",
    "    df = cap_outliers_iqr(df, column)\n",
    "\n",
    "# Debug: Show the updated data after capping outliers\n",
    "print(\"\\nData after capping outliers:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, df.shape\n",
    "print(\n",
    "    f\"Original data shape: {data.shape}\\nDataframe shape after processing: {df.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import powerlaw\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class PlaycountGenerator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.n_tracks = len(df)  # Use total number of rows instead of unique tracks\n",
    "\n",
    "    def power_law(self, alpha=2.0, xmin=1):\n",
    "        playcounts_raw = powerlaw.Power_Law(\n",
    "            xmin=xmin, parameters=[alpha]\n",
    "        ).generate_random(self.n_tracks)\n",
    "        return np.round(playcounts_raw).astype(int)\n",
    "\n",
    "    def log_normal(self, mu=5, sigma=1.5):\n",
    "        return np.round(\n",
    "            np.exp(stats.norm.rvs(loc=mu, scale=sigma, size=self.n_tracks))\n",
    "        ).astype(int)\n",
    "\n",
    "    def combined_factors(\n",
    "        self,\n",
    "        energy_factor_weight=0.5,\n",
    "        danceability_factor_weight=0.3,\n",
    "        alpha=2.0,\n",
    "        xmin=1,\n",
    "    ):\n",
    "        base_playcounts = np.round(\n",
    "            powerlaw.Power_Law(xmin=xmin, parameters=[alpha]).generate_random(\n",
    "                self.n_tracks\n",
    "            )\n",
    "        ).astype(int)\n",
    "        energy_factor = (\n",
    "            1\n",
    "            + (self.df[\"energy\"] - self.df[\"energy\"].min())\n",
    "            / (self.df[\"energy\"].max() - self.df[\"energy\"].min())\n",
    "            * energy_factor_weight\n",
    "        )\n",
    "        danceability_factor = (\n",
    "            1\n",
    "            + (self.df[\"danceability\"] - self.df[\"danceability\"].min())\n",
    "            / (self.df[\"danceability\"].max() - self.df[\"danceability\"].min())\n",
    "            * danceability_factor_weight\n",
    "        )\n",
    "        return np.round(base_playcounts * energy_factor * danceability_factor).astype(\n",
    "            int\n",
    "        )\n",
    "\n",
    "    def negative_binomial(self, n=5, p=0.3):\n",
    "        return stats.nbinom.rvs(n, p, size=self.n_tracks)\n",
    "\n",
    "    def add_noise(self, playcounts, noise_std=0.2):\n",
    "        noise = np.random.normal(0, noise_std, self.n_tracks)\n",
    "        return np.round(playcounts * (1 + noise)).astype(int)\n",
    "\n",
    "    def evaluate(self, playcounts, method_name):\n",
    "        self.df[\"playcount\"] = playcounts\n",
    "        print(f\"Evaluation for {method_name}:\")\n",
    "        print(\n",
    "            self.df[[\"age\", \"duration\", \"energy\", \"danceability\", \"playcount\"]].corr()\n",
    "        )  # correlation with other features\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(self.df[\"playcount\"], kde=True)\n",
    "        plt.title(f\"{method_name} Playcount Distribution\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(self.df.index, self.df[\"playcount\"])\n",
    "        plt.title(f\"{method_name} Playcount Scatter Plot\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your DataFrame 'df'\n",
    "generator = PlaycountGenerator(df)\n",
    "\n",
    "# Generate playcounts using different methods\n",
    "playcounts_powerlaw = generator.power_law()\n",
    "playcounts_lognormal = generator.log_normal()\n",
    "playcounts_combined = generator.combined_factors()\n",
    "playcounts_negbin = generator.negative_binomial()\n",
    "\n",
    "# Evaluate each method\n",
    "generator.evaluate(playcounts_powerlaw, \"Power Law\")\n",
    "generator.evaluate(playcounts_lognormal, \"Log Normal\")\n",
    "generator.evaluate(playcounts_combined, \"Combined Factors\")\n",
    "generator.evaluate(playcounts_negbin, \"Negative Binomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "class PlaycountGenerator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.features = [\n",
    "            \"age\",\n",
    "            \"duration\",\n",
    "            \"energy\",\n",
    "            \"danceability\",\n",
    "            \"acousticness\",\n",
    "            \"valence\",\n",
    "            \"tempo\",\n",
    "        ]\n",
    "\n",
    "    def analyze_correlations(self):\n",
    "        \"\"\"\n",
    "        Analyze correlations between features and existing playcounts\n",
    "        Returns correlation matrix and plots heatmap\n",
    "        \"\"\"\n",
    "        corr_matrix = self.df[self.features + [\"playcount\"]].corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "        plt.title(\"Feature Correlations with Playcount\")\n",
    "        plt.show()\n",
    "        return corr_matrix\n",
    "\n",
    "    def generate_poisson(self, lambda_param=10):\n",
    "        \"\"\"\n",
    "        Generate playcounts using Poisson distribution\n",
    "        Params:\n",
    "            lambda_param: Mean playcount value\n",
    "        \"\"\"\n",
    "        return np.random.poisson(lambda_param, len(self.df))\n",
    "\n",
    "    def generate_negative_binomial(self, n=5, p=0.5):\n",
    "        \"\"\"\n",
    "        Generate playcounts using Negative Binomial\n",
    "        Useful for overdispersed count data\n",
    "        \"\"\"\n",
    "        return np.random.negative_binomial(n, p, len(self.df))\n",
    "\n",
    "    def generate_zero_inflated(self, lambda_param=10, zero_prob=0.2):\n",
    "        \"\"\"\n",
    "        Generate zero-inflated playcounts\n",
    "        Combines zeros with Poisson distribution\n",
    "        \"\"\"\n",
    "        zeros = np.random.binomial(1, zero_prob, len(self.df))\n",
    "        counts = self.generate_poisson(lambda_param)\n",
    "        return counts * (1 - zeros)\n",
    "\n",
    "    def generate_feature_weighted(self):\n",
    "        \"\"\"\n",
    "        Generate playcounts weighted by feature correlations\n",
    "        Uses feature importance to influence playcount distribution\n",
    "        \"\"\"\n",
    "        features_norm = stats.zscore(self.df[self.features])\n",
    "        weights = np.abs(np.corrcoef(features_norm.T)[-1])\n",
    "        base_counts = np.random.poisson(10, len(self.df))\n",
    "        weighted_counts = base_counts * (1 + 0.2 * features_norm.mean(axis=1))\n",
    "        return np.maximum(weighted_counts, 0).astype(int)\n",
    "\n",
    "    def plot_distributions(self, n_samples=1000):\n",
    "        \"\"\"\n",
    "        Plot different playcount distributions\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        distributions = {\n",
    "            \"Poisson\": self.generate_poisson(),\n",
    "            \"Negative Binomial\": self.generate_negative_binomial(),\n",
    "            \"Zero-inflated\": self.generate_zero_inflated(),\n",
    "            \"Feature-weighted\": self.generate_feature_weighted(),\n",
    "        }\n",
    "\n",
    "        for (title, counts), ax in zip(distributions.items(), axes.ravel()):\n",
    "            sns.histplot(counts, ax=ax)\n",
    "            ax.set_title(f\"{title} Distribution\")\n",
    "            ax.set_xlabel(\"Playcount\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return distributions\n",
    "\n",
    "# Usage\n",
    "generator = PlaycountGenerator(df)\n",
    "correlations = generator.analyze_correlations()\n",
    "distributions = generator.plot_distributions()\n",
    "\n",
    "# Select best distribution based on data characteristics\n",
    "df[\"synthetic_playcount\"] = generator.generate_feature_weighted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaycountGenerator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.features = ['age', 'duration', 'energy', 'danceability', \n",
    "                        'acousticness', 'valence', 'tempo']\n",
    "\n",
    "    def analyze_correlations(self):\n",
    "        \"\"\"\n",
    "        Analyze correlations between features and existing playcounts\n",
    "        Returns correlation matrix and plots heatmap\n",
    "        \"\"\"\n",
    "        corr_matrix = self.df[self.features + ['playcount']].corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "        plt.title('Feature Correlations with Playcount')\n",
    "        plt.show()\n",
    "        return corr_matrix\n",
    "    \n",
    "    def generate_poisson(self, lambda_param=10):\n",
    "        \"\"\"\n",
    "        Generate playcounts using Poisson distribution\n",
    "        Params:\n",
    "            lambda_param: Mean playcount value\n",
    "        \"\"\"\n",
    "        return np.random.poisson(lambda_param, len(self.df))\n",
    "    \n",
    "    def generate_negative_binomial(self, n=5, p=0.5):\n",
    "        \"\"\"\n",
    "        Generate playcounts using Negative Binomial\n",
    "        Useful for overdispersed count data\n",
    "        \"\"\"\n",
    "        return np.random.negative_binomial(n, p, len(self.df))\n",
    "    \n",
    "    def generate_zero_inflated(self, lambda_param=10, zero_prob=0.2):\n",
    "        \"\"\"\n",
    "        Generate zero-inflated playcounts\n",
    "        Combines zeros with Poisson distribution\n",
    "        \"\"\"\n",
    "        zeros = np.random.binomial(1, zero_prob, len(self.df))\n",
    "        counts = self.generate_poisson(lambda_param)\n",
    "        return counts * (1 - zeros)\n",
    "    \n",
    "    def generate_feature_weighted(self):\n",
    "        \"\"\"\n",
    "        Generate playcounts weighted by feature correlations\n",
    "        Uses feature importance to influence playcount distribution\n",
    "        \"\"\"\n",
    "        features_norm = stats.zscore(self.df[self.features])\n",
    "        weights = np.abs(np.corrcoef(features_norm.T)[-1])\n",
    "        base_counts = np.random.poisson(10, len(self.df))\n",
    "        weighted_counts = base_counts * (1 + 0.2 * features_norm.mean(axis=1))\n",
    "        return np.maximum(weighted_counts, 0).astype(int)\n",
    "    \n",
    "    def generate_user_aware_counts(self):\n",
    "        \"\"\"\n",
    "        Generate playcounts considering user behavior patterns\n",
    "        \"\"\"\n",
    "        # User activity factors (some users listen more than others)\n",
    "        user_factors = np.random.normal(1, 0.2, len(np.unique(self.df['user_id'])))\n",
    "        \n",
    "        # Base song popularity using feature weights\n",
    "        features_norm = stats.zscore(self.df[self.features])\n",
    "        weights = np.abs(np.corrcoef(features_norm.T)[-1])\n",
    "        base_counts = np.random.poisson(10, len(self.df))\n",
    "        song_popularity = base_counts * (1 + 0.2 * features_norm.mean(axis=1))\n",
    "        \n",
    "        # Map user factors to each song play\n",
    "        user_indices = pd.factorize(self.df['user_id'])[0]\n",
    "        weighted_counts = (song_popularity * user_factors[user_indices])\n",
    "        \n",
    "        return np.maximum(weighted_counts, 0).astype(int)\n",
    "    \n",
    "    def plot_comparison(self):\n",
    "        \"\"\"\n",
    "        Compare original and user-aware playcount distributions\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        original_counts = self.generate_feature_weighted()\n",
    "        user_aware_counts = self.generate_user_aware_counts()\n",
    "        \n",
    "        sns.histplot(original_counts, ax=ax1)\n",
    "        ax1.set_title('Feature-weighted Distribution')\n",
    "        ax1.set_xlabel('Playcount')\n",
    "        \n",
    "        sns.histplot(user_aware_counts, ax=ax2)\n",
    "        ax2.set_title('User-aware Distribution')\n",
    "        ax2.set_xlabel('Playcount')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return original_counts, user_aware_counts\n",
    "\n",
    "# Test the new implementation\n",
    "generator = PlaycountGenerator(df)\n",
    "original, user_aware = generator.plot_comparison()\n",
    "\n",
    "# Basic statistics comparison\n",
    "print(\"\\nDistribution Statistics:\")\n",
    "print(pd.DataFrame({\n",
    "    'Original': [original.mean(), original.std(), np.percentile(original, 95)],\n",
    "    'User-aware': [user_aware.mean(), user_aware.std(), np.percentile(user_aware, 95)]\n",
    "}, index=['Mean', 'Std', '95th percentile']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaycountGenerator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.features = ['duration', 'energy', 'danceability', \n",
    "                        'acousticness', 'valence', 'tempo']\n",
    "    \n",
    "    def analyze_correlations(self):\n",
    "        \"\"\"\n",
    "        Analyze correlations between features and existing playcounts\n",
    "        Returns correlation matrix and plots heatmap\n",
    "        \"\"\"\n",
    "        corr_matrix = self.df[self.features + ['playcount']].corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "        plt.title('Feature Correlations with Playcount')\n",
    "        plt.show()\n",
    "        return corr_matrix\n",
    "    \n",
    "    def generate_poisson(self, lambda_param=10):\n",
    "        \"\"\"\n",
    "        Generate playcounts using Poisson distribution\n",
    "        Params:\n",
    "            lambda_param: Mean playcount value\n",
    "        \"\"\"\n",
    "        return np.random.poisson(lambda_param, len(self.df))\n",
    "    \n",
    "    def generate_negative_binomial(self, n=5, p=0.5):\n",
    "        \"\"\"\n",
    "        Generate playcounts using Negative Binomial\n",
    "        Useful for overdispersed count data\n",
    "        \"\"\"\n",
    "        return np.random.negative_binomial(n, p, len(self.df))\n",
    "    \n",
    "    def generate_zero_inflated(self, lambda_param=10, zero_prob=0.2):\n",
    "        \"\"\"\n",
    "        Generate zero-inflated playcounts\n",
    "        Combines zeros with Poisson distribution\n",
    "        \"\"\"\n",
    "        zeros = np.random.binomial(1, zero_prob, len(self.df))\n",
    "        counts = self.generate_poisson(lambda_param)\n",
    "        return counts * (1 - zeros)\n",
    "    \n",
    "    def generate_feature_weighted(self):\n",
    "        \"\"\"\n",
    "        Generate playcounts weighted by feature correlations\n",
    "        Uses feature importance to influence playcount distribution\n",
    "        \"\"\"\n",
    "        features_norm = stats.zscore(self.df[self.features])\n",
    "        weights = np.abs(np.corrcoef(features_norm.T)[-1])\n",
    "        base_counts = np.random.poisson(10, len(self.df))\n",
    "        weighted_counts = base_counts * (1 + 0.2 * features_norm.mean(axis=1))\n",
    "        return np.maximum(weighted_counts, 0).astype(int)\n",
    "    \n",
    "    def generate_user_aware_counts(self):\n",
    "        \"\"\"\n",
    "        Generate playcounts considering user behavior patterns\n",
    "        \"\"\"\n",
    "        # User activity factors (some users listen more than others)\n",
    "        user_factors = np.random.normal(1, 0.2, len(np.unique(self.df['user_id'])))\n",
    "        \n",
    "        # Base song popularity using feature weights\n",
    "        features_norm = stats.zscore(self.df[self.features])\n",
    "        weights = np.abs(np.corrcoef(features_norm.T)[-1])\n",
    "        base_counts = np.random.poisson(10, len(self.df))\n",
    "        song_popularity = base_counts * (1 + 0.2 * features_norm.mean(axis=1))\n",
    "        \n",
    "        # Map user factors to each song play\n",
    "        user_indices = pd.factorize(self.df['user_id'])[0]\n",
    "        weighted_counts = (song_popularity * user_factors[user_indices])\n",
    "        \n",
    "        return np.maximum(weighted_counts, 0).astype(int)\n",
    "    \n",
    "    def generate_enhanced_playcounts(self):\n",
    "        # Age factor\n",
    "        current_date = pd.Timestamp.now()\n",
    "        release_dates = pd.to_datetime(self.df['age'])\n",
    "        song_age = (current_date - release_dates).dt.days / 365.25\n",
    "        age_factor = np.exp(-0.5 * song_age)\n",
    "        \n",
    "        # Genre popularity\n",
    "        genre_popularity = self.df.groupby('main_genre')['playcount'].mean()\n",
    "        genre_factor = self.df['main_genre'].map(genre_popularity)\n",
    "        genre_factor = (genre_factor - genre_factor.min()) / (genre_factor.max() - genre_factor.min())\n",
    "        \n",
    "        # User factors\n",
    "        user_factors = np.random.normal(1, 0.2, len(np.unique(self.df['user_id'])))\n",
    "        user_indices = pd.factorize(self.df['user_id'])[0]\n",
    "        \n",
    "        # Base counts with features\n",
    "        features_norm = stats.zscore(self.df[self.features])\n",
    "        base_counts = np.random.poisson(10, len(self.df))\n",
    "        song_popularity = base_counts * (1 + 0.2 * features_norm.mean(axis=1))\n",
    "        \n",
    "        # Combine all factors\n",
    "        final_counts = (song_popularity * \n",
    "                       user_factors[user_indices] * \n",
    "                       (1 + 0.3 * age_factor) * \n",
    "                       (1 + 0.2 * genre_factor))\n",
    "        \n",
    "        return np.maximum(final_counts, 0).astype(int)\n",
    "    \n",
    "    def plot_comparison(self):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "        \n",
    "        base_counts = self.generate_feature_weighted()\n",
    "        user_aware = self.generate_user_aware_counts()\n",
    "        enhanced = self.generate_enhanced_playcounts()\n",
    "        \n",
    "        distributions = {\n",
    "            'Base': base_counts,\n",
    "            'User-aware': user_aware,\n",
    "            'Enhanced': enhanced\n",
    "        }\n",
    "        \n",
    "        for (title, counts), ax in zip(distributions.items(), axes):\n",
    "            sns.histplot(counts, ax=ax)\n",
    "            ax.set_title(f'{title} Distribution')\n",
    "            ax.set_xlabel('Playcount')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Statistics comparison\n",
    "        stats_df = pd.DataFrame({\n",
    "            'Base': [base_counts.mean(), base_counts.std(), np.percentile(base_counts, 95)],\n",
    "            'User-aware': [user_aware.mean(), user_aware.std(), np.percentile(user_aware, 95)],\n",
    "            'Enhanced': [enhanced.mean(), enhanced.std(), np.percentile(enhanced, 95)]\n",
    "        }, index=['Mean', 'Std', '95th percentile'])\n",
    "        \n",
    "        return stats_df\n",
    "\n",
    "# Run comparison\n",
    "generator = PlaycountGenerator(df)\n",
    "comparison_stats = generator.plot_comparison()\n",
    "print(\"\\nDistribution Statistics:\")\n",
    "print(comparison_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"enhanced_playcount\"] = generator.generate_enhanced_playcounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'enhanced_playcount' in df.columns:\n",
    "    aggregated_data = df.groupby(['user_id', 'music_id']).agg({\"enhanced_playcount\": 'sum'}).reset_index()\n",
    "    \n",
    "aggregated_data.head()\n",
    "    \n",
    "# apply to the original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(aggregated_data, on=['user_id', 'music_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"enhanced_playcount_x\",\"plays\"], inplace=True)\n",
    "df.rename(columns={\"enhanced_playcount_y\": \"playcount\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/o1_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/o1_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in the data\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate entries: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_duplicates = df[df.duplicated(keep=False)]\n",
    "print(f\"Number of exact duplicates: {exact_duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert if duplicates are gone\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate entries: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first playcount column, should be next to synthetic_playcount\n",
    "df.drop(columns=[\"playcount\",\"synthetic_playcount\"], inplace=True)\n",
    "# rename id_artist to artist_id\n",
    "df.rename(columns={\"id_artist\": \"artist_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename playcount.1 to playcount\n",
    "df.rename(columns={\"playcount.1\": \"playcount\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename id_artists to artist_id\n",
    "df.rename(columns={\"id_artists\": \"artist_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for equal name/id matches in the data\n",
    "names =['artist_name', 'music']\n",
    "ids = ['artist_id', 'music_id']\n",
    "\n",
    "def check_name_id_match(df, names, ids):\n",
    "    for name, id in zip(names, ids):\n",
    "        name_id_match = df.groupby(name)[id].nunique()\n",
    "        if name_id_match.max() > 1:\n",
    "            print(f\"Warning: {name} has multiple {id} entries.\")\n",
    "        else:\n",
    "            print(f\"Name and {id} match is consistent.\")\n",
    "\n",
    "check_name_id_match(df, names, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix artist_name and artist_id mismatch\n",
    "duplicate_artists = df['artist_name'].duplicated().sum()\n",
    "duplicate_artist_ids = df['artist_id'].duplicated().sum()\n",
    "print(f'Number of duplicate artist names: {duplicate_artists}\\nNumber of duplicate artist IDs: {duplicate_artist_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hashing to encode artist names\n",
    "df[\"artist_id\"] = df[\"artist_name\"].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/o2_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicPreprocessor:\n",
    "    def __init__(self, handle_outliers=\"robust\"):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor\n",
    "\n",
    "        Parameters:\n",
    "        handle_outliers (str): Strategy for handling outliers\n",
    "            'robust': Use RobustScaler\n",
    "            'cap': Use IQR-based capping\n",
    "            'none': Leave outliers as is\n",
    "        \"\"\"\n",
    "        self.handle_outliers = handle_outliers\n",
    "        self.scalers = {}\n",
    "\n",
    "    def _cap_outliers(self, series):\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return np.clip(series, lower_bound, upper_bound)\n",
    "\n",
    "    def _handle_time_signature_outliers(self, df):\n",
    "        # Example: Filter out time signatures that are not in the expected range\n",
    "        expected_time_signatures = [3, 4]\n",
    "        df = df[df[\"time_signature\"].isin(expected_time_signatures)]\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Preprocess the music dataset\"\"\"\n",
    "        df_processed = df.copy()\n",
    "\n",
    "        # 1. Handle temporal features\n",
    "        if \"release_year\" in df_processed.columns:\n",
    "            df_processed[\"music_age\"] = 2024 - df_processed[\"release_year\"]\n",
    "\n",
    "        # 2. Handle plays (use log transformation due to heavy skew)\n",
    "        # if \"plays\" in df_processed.columns:\n",
    "        #     df_processed[\"plays_log\"] = np.log1p(df_processed[\"plays\"])\n",
    "\n",
    "        # 3. Handle time signature outliers\n",
    "        df_processed = self._handle_time_signature_outliers(df_processed)\n",
    "\n",
    "        # 4. Process audio features\n",
    "        audio_features = [\n",
    "            \"acousticness\",\n",
    "            \"danceability\",\n",
    "            \"energy\",\n",
    "            \"instrumentalness\",\n",
    "            \"liveness\",\n",
    "            \"loudness\",\n",
    "            \"speechiness\",\n",
    "            \"valence\",\n",
    "            \"tempo\",\n",
    "        ]\n",
    "\n",
    "        for feature in audio_features:\n",
    "            if feature in df_processed.columns:\n",
    "                if self.handle_outliers == \"cap\":\n",
    "                    df_processed[feature] = self._cap_outliers(df_processed[feature])\n",
    "                elif self.handle_outliers == \"robust\":\n",
    "                    self.scalers[feature] = RobustScaler()\n",
    "                    df_processed[feature] = self.scalers[feature].fit_transform(\n",
    "                        df_processed[feature].values.reshape(-1, 1)\n",
    "                    )\n",
    "\n",
    "        # 5. Create interaction features\n",
    "        df_processed[\"energy_loudness\"] = (\n",
    "            df_processed[\"energy\"] * df_processed[\"loudness\"]\n",
    "        )\n",
    "        df_processed[\"dance_valence\"] = (\n",
    "            df_processed[\"danceability\"] * df_processed[\"valence\"]\n",
    "        )\n",
    "\n",
    "        # 6. Drop highly correlated features to reduce multicollinearity\n",
    "        # (optional, depending on your model type)\n",
    "        high_corr_features = [\"loudness\"]  # Since it's highly correlated with energy\n",
    "        df_processed = df_processed.drop(high_corr_features, axis=1, errors=\"ignore\")\n",
    "\n",
    "        return df_processed\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform new data using fitted preprocessor\"\"\"\n",
    "        df_processed = df.copy()\n",
    "\n",
    "        if \"release_year\" in df_processed.columns:\n",
    "            df_processed[\"music_age\"] = 2024 - df_processed[\"release_year\"]\n",
    "\n",
    "        # if \"plays\" in df_processed.columns:\n",
    "        #     df_processed[\"plays_log\"] = np.log1p(df_processed[\"plays\"])\n",
    "\n",
    "        for feature, scaler in self.scalers.items():\n",
    "            if feature in df_processed.columns:\n",
    "                df_processed[feature] = scaler.transform(\n",
    "                    df_processed[feature].values.reshape(-1, 1)\n",
    "                )\n",
    "\n",
    "        df_processed[\"energy_loudness\"] = (\n",
    "            df_processed[\"energy\"] * df_processed[\"loudness\"]\n",
    "        )\n",
    "        df_processed[\"dance_valence\"] = (\n",
    "            df_processed[\"danceability\"] * df_processed[\"valence\"]\n",
    "        )\n",
    "\n",
    "        return df_processed.drop([\"loudness\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "\n",
    "# Assuming df is already defined and contains the cleaned data from previous steps\n",
    "\n",
    "# Initialize the preprocessor with 'robust' outlier handling\n",
    "preprocessor = MusicPreprocessor(handle_outliers=\"robust\")\n",
    "\n",
    "# Fit and transform the data\n",
    "df_processed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Show the processed data\n",
    "print(df_processed.head())\n",
    "\n",
    "# Visualize the distribution before and after transformation\n",
    "\n",
    "# Original 'plays' distribution\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.histplot(df[\"plays\"], bins=10, kde=True)\n",
    "# plt.title(\"Original Plays Distribution\")\n",
    "\n",
    "# Transformed 'plays_log' distribution\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.histplot(df_processed[\"plays_log\"], bins=10, kde=True)\n",
    "# plt.title(\"Transformed Plays Log Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Original 'energy' distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df[\"energy\"], bins=10, kde=True)\n",
    "plt.title(\"Original Energy Distribution\")\n",
    "\n",
    "# Transformed 'energy' distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_processed[\"energy\"], bins=10, kde=True)\n",
    "plt.title(\"Transformed Energy Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of time_signature before and after transformation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=\"time_signature\", data=df)\n",
    "plt.title(\"Original Time Signature Distribution\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=\"time_signature\", data=df_processed)\n",
    "plt.title(\"Processed Time Signature Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"synthetic_playcount\" in df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use robust scaling on \"plays\" and plot for visualization using seaborn\n",
    "scaler = RobustScaler()\n",
    "df[\"plays_scaled\"] = scaler.fit_transform(df[[\"synthetic_playcount\"]])\n",
    "sns.histplot(df[\"plays_scaled\"], bins=10, kde=True)\n",
    "plt.title(\"Robust Scaled Plays Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed[\"plays_log\"]\n",
    "sns.histplot(df_processed[\"plays_log\"], bins=10, kde=True)\n",
    "plt.title(\"Log Transformed Plays Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # handle missing values in 'genre' and 'featured_artists'\n",
    "# df_processed[\"\"].replace(\"Unknown\", \"Other\", inplace=True)\n",
    "# df_processed[\"featured_artists\"].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make datframe of all unknown values in 'genre' and 'featured_artists' along with the music\n",
    "# unknown_genre = df_processed[df_processed[\"genre\"] == \"Other\"]\n",
    "# unknown_artists = df_processed[df_processed[\"featured_artists\"] == \"NaN\"]\n",
    "# unknown_artists\n",
    "# # unknown_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that artist names and artist ids are unique\n",
    "df_processed[\"artist_id\"].nunique(), df_processed[\"artist_name\"].nunique()\n",
    "print(\n",
    "    f'Number of unique artists: {df_processed[\"artist_id\"].nunique()}\\nNumber of unique artist names: {df_processed[\"artist_name\"].nunique()}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'main_genre' in df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/o2_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"main_genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lhydra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
