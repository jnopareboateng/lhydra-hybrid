# Lhydra Hybrid Music Recommender System Configuration

# Model configuration
model:
  embedding_dim: 32 # Dimension of embedding vectors
  hidden_dims: [128, 64, 32] # Hidden layer dimensions for towers
  prediction_dims: [64, 32] # Hidden layer dimensions for prediction layers
  dropout: 0.2 # Dropout rate
  final_layer_size: 16 # Size of final layer before prediction
  user_tower:
    hidden_layers: [128, 64, 32] # Hidden layer dimensions for user tower
    dropout: 0.2 # Dropout rate for user tower
    activation: "relu" # Activation function for user tower
  item_tower:
    hidden_layers: [128, 64, 32] # Hidden layer dimensions for item tower
    dropout: 0.2 # Dropout rate for item tower
    activation: "relu" # Activation function for item tower

# Training configuration
training:
  learning_rate: 0.001 # Initial learning rate
  weight_decay: 1e-4 # Weight decay for regularization
  num_epochs: 50 # Maximum number of epochs
  patience: 5 # Early stopping patience
  batch_size: 128 # Batch size
  num_workers: 4 # Number of workers for data loading
  checkpoint_dir: "models/checkpoints" # Directory to save checkpoints
  log_dir: "logs" # Directory to save logs
  optimizer: "adam" # Optimizer to use
  loss_function: "binary_cross_entropy" # Loss function to use
  class_weights: [0.3, 0.7] # [negative_weight, positive_weight]
  lr_scheduler:
    use: true # Whether to use learning rate scheduler
    factor: 0.5 # Factor to decrease learning rate by
    patience: 3 # Patience for lr scheduler
  early_stopping_patience: 5 # Patience for early stopping

# Data configuration
data:
  target_column: "high_engagement" # Target column name

  # Data paths (can be overridden by command line args)
  train_path: "../notebooks/ship_test_1k_data/train_data.csv" # Path to interactions file
  validation_path: "../notebooks/ship_test_1k_data/val_data.csv" # Path to validation data
  test_path: "../notebooks/ship_test_1k_data/test_data.csv" # Path to test data
  preprocessed_data_dir: "../notebooks/ship_test_1k_data/preprocessed" # Directory containing preprocessed data
  user_features_path: "data/user_features.csv" # Path to user features file (optional)
  item_features_path: "data/item_features.csv" # Path to item features file (optional)

  # User features
  user_demographic_features: # User demographic features
    - "age"
    - "gender"
    - "country"
  user_listening_features: # User listening behavior features
    - "monthly_hours"
    - "genre_diversity"
    - "top_genre"
  user_audio_preferences: # User audio preferences
    - "avg_danceability"
    - "avg_energy"
    - "avg_key"
    - "avg_loudness"
    - "avg_mode"
    - "avg_speechiness"
    - "avg_acousticness"
    - "avg_instrumentalness"
    - "avg_liveness"
    - "avg_valence"
    - "avg_tempo"
    - "avg_time_signature"

  # Track features
  track_metadata_features: # Track metadata features
    - "artist"
    - "main_genre"
    - "year"
    - "duration_ms"
  track_audio_features: # Track audio features
    - "danceability"
    - "energy"
    - "key"
    - "loudness"
    - "mode"
    - "speechiness"
    - "acousticness"
    - "instrumentalness"
    - "liveness"
    - "valence"
    - "tempo"
    - "time_signature"

  # For backward compatibility
  categorical_features: # Categorical feature columns
    - "gender"
    - "country"
    - "top_genre"
    - "main_genre"
    - "artist"
  numerical_features: # Numerical feature columns
    - "age"
    - "monthly_hours"
    - "genre_diversity"
    - "year"
    - "duration_ms"

  # Data processing configuration
  target_threshold: 5 # Threshold for high engagement
  test_size: 0.2 # Test set size
  validation_size: 0.1 # Validation set size
  random_seed: 42 # Random seed
  embedding_dim: 32 # Dimension for embedding layers

# Evaluation configuration
evaluation:
  metrics: ["accuracy", "precision", "recall", "f1", "auc", "map", "ndcg"]
  k_values: [5, 10, 20, 50, 100] # k values for ranking metrics
  save_predictions: true # Whether to save predictions
  save_metrics: true # Whether to save metrics
  cohort_analysis: true # Whether to perform cohort analysis
  metrics_dir: "evaluation/metrics" # Directory to save metrics
  predictions_dir: "evaluation/predictions" # Directory to save predictions

# Logging configuration
logging:
  log_dir: "logs" # Directory to save logs
  tensorboard: true # Whether to use tensorboard
  log_frequency: 100 # Frequency of logging
  model_checkpoint_dir: "models/checkpoints" # Directory to save model checkpoints
  save_best_only: true # Whether to save only the best model

# Inference configuration
inference:
  top_k_recommendations: 10 # Number of recommendations to generate
  batch_size: 256 # Batch size for inference
  output_dir: "recommendations/" # Directory to save recommendations
